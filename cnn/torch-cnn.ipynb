{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "# Set the device\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        filter_size: int,\n",
    "        n_filters: int,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        input_dim=(1, 28, 28),\n",
    "        activation=torch.relu,\n",
    "    ) -> None:\n",
    "        \"\"\"Create a Convolutional Layer\n",
    "\n",
    "        Args:\n",
    "            filter_size (int): Filter Size. Eg: 5 = 5 x 5\n",
    "            n_filters (int): Number of filters.\n",
    "            stride (int): Stride. Defaults to 1\n",
    "            padding (int): Padding. Pad of 1 adds pad all around the image. Defaults to 1\n",
    "            input_dim (tuple): Channels * Height * Breadth. Defaults to (1, 28, 28)\n",
    "        \"\"\"\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.input_dim = input_dim\n",
    "        self.filter_size = filter_size\n",
    "        self.n_filters = n_filters\n",
    "        self.params_initialized = False\n",
    "        self.filters = []\n",
    "        self.filter_biases = []\n",
    "        \n",
    "    def initialize_params(self) -> None:\n",
    "        \"\"\"Initialize Params\n",
    "        \"\"\"\n",
    "        # Create filters\n",
    "        self.filters = [\n",
    "            torch.randn(\n",
    "                self.input_dim[0],\n",
    "                self.filter_size,\n",
    "                self.filter_size,\n",
    "                requires_grad=True,\n",
    "                dtype=torch.float32,\n",
    "                device=device\n",
    "            )\n",
    "            for _ in range(self.n_filters)\n",
    "        ]\n",
    "        self.filter_biases = [\n",
    "            torch.randn(1, requires_grad=True, dtype=torch.float32, device=device)\n",
    "            for _ in range(self.n_filters)\n",
    "        ]\n",
    "        self.params_initialized = True\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_img_tensor(img_tensor: torch.tensor, padding: int) -> torch.tensor:\n",
    "        \"\"\"Pad Image Tensor Object\n",
    "\n",
    "        Args:\n",
    "            img_tensor (torch.tensor): Image Tensor to pad with\n",
    "            padding (int): Padding for the tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.tensor: Padded tensor\n",
    "        \"\"\"\n",
    "        img_tensor_shape = img_tensor.shape\n",
    "        padded_tensor = torch.zeros(\n",
    "            img_tensor_shape[0],\n",
    "            img_tensor_shape[1] + 2 * padding,\n",
    "            img_tensor_shape[2] + 2 * padding,\n",
    "            device=device\n",
    "        )\n",
    "        for channel_idx in range(img_tensor_shape[0]):\n",
    "            for row_idx in range(img_tensor_shape[1]):\n",
    "                padded_tensor[channel_idx, row_idx + 1, 1:-1] = img_tensor[\n",
    "                    channel_idx, row_idx\n",
    "                ]\n",
    "        return padded_tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def tensor_to_image(img_tensor: torch.tensor) -> T.ToPILImage:\n",
    "        \"\"\"Conver tensor to a PIL image for visualization\n",
    "\n",
    "        Args:\n",
    "            img_tensor (torch.tensor): Image Tensor\n",
    "\n",
    "        Returns:\n",
    "            IMAGE: PIL Image\n",
    "        \"\"\"\n",
    "        transform = T.ToPILImage()\n",
    "        img = transform(img_tensor)\n",
    "        return img\n",
    "\n",
    "    def get_model_params(self) -> list:\n",
    "        \"\"\"Returns a list of Model Params\n",
    "\n",
    "        Returns:\n",
    "            list: List of Model params\n",
    "        \"\"\"\n",
    "        return [self.filters, self.filter_biases]\n",
    "\n",
    "    def forward_pass(self, input_tensor: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"Forward Pass for the layer (Convolution over the image)\n",
    "\n",
    "        Args:\n",
    "            input_tensor (torch.tensor): Image Tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.tensor: Activation map of the layer.\n",
    "        \"\"\"\n",
    "        if not self.params_initialized:\n",
    "            self.initialize_params()\n",
    "\n",
    "        if self.padding > 0:\n",
    "            input_tensor = self.pad_img_tensor(input_tensor, self.padding)\n",
    "\n",
    "        img_channels = self.input_dim[0]\n",
    "        img_size = self.input_dim[1] + 2 * self.padding # Padding is also included now\n",
    "        output_size = ((img_size - self.filter_size) / self.stride) + 1\n",
    "        \n",
    "        # Return None if unsupported\n",
    "        if output_size % 1 != 0:\n",
    "            print(\"Kernel Size not applicable.\")\n",
    "            return None\n",
    "\n",
    "        output_size = int(output_size)\n",
    "        activation_map = torch.zeros(\n",
    "            len(self.filters), output_size, output_size, device=device\n",
    "        )\n",
    "\n",
    "        for filter_idx in range(len(self.filters)):\n",
    "            for row_idx in range(\n",
    "                0, img_size - self.filter_size + 1, self.stride\n",
    "            ):\n",
    "                for col_idx in range(\n",
    "                    0, img_size - self.filter_size + 1, self.stride\n",
    "                ):\n",
    "                    flatten_size = (\n",
    "                        img_channels * self.filter_size * self.filter_size\n",
    "                    )\n",
    "                    activation_map[\n",
    "                        filter_idx, row_idx, col_idx\n",
    "                    ] = torch.relu(\n",
    "                        input_tensor[\n",
    "                            :,\n",
    "                            row_idx : row_idx + self.filter_size,\n",
    "                            col_idx : col_idx + self.filter_size,\n",
    "                        ].reshape(1, flatten_size)\n",
    "                        @ self.filters[filter_idx].reshape(flatten_size, 1)\n",
    "                        + self.filter_biases[filter_idx]\n",
    "                    )\n",
    "        return activation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Layer\n",
    "class SequentialLayer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        neurons_per_layer: int,\n",
    "        activation=torch.tanh,\n",
    "        n_inputs_per_neuron=0,\n",
    "    ) -> None:\n",
    "        \"\"\"Sequential Layer\n",
    "\n",
    "        Args:\n",
    "            neurons_per_layer (int): Neurons in the layer\n",
    "            activation (torch.<ActivationFunction>, None): Activation Function for Layer.\n",
    "                Defaults to torch.tanh. Set None for no activation\n",
    "            n_inputs_per_neuron (int): Inputs per neuron in the layer\n",
    "        \"\"\"\n",
    "        self.n_inputs_per_neuron = n_inputs_per_neuron\n",
    "        self.neurons_per_layer = neurons_per_layer\n",
    "        self.activation = activation\n",
    "        self.params_initialized = False\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "\n",
    "    def initialize_params(self) -> None:\n",
    "        \"\"\"Initialize Model Params\"\"\"\n",
    "        self.weights = torch.randn(\n",
    "            self.neurons_per_layer,\n",
    "            self.n_inputs_per_neuron,\n",
    "            requires_grad=True,\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        self.biases = torch.randn(\n",
    "            self.neurons_per_layer, 1, requires_grad=True, dtype=torch.float32\n",
    "        )\n",
    "        self.params_initialized = True\n",
    "\n",
    "    def return_model_params(self) -> list:\n",
    "        \"\"\"Returns a list of model params\n",
    "\n",
    "        Returns:\n",
    "            list: List of model params\n",
    "        \"\"\"\n",
    "        return [self.weights, self.biases]\n",
    "\n",
    "    def forward_pass(self, x: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"Forward Pass of the Layer\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): Input to the layer\n",
    "\n",
    "        Returns:\n",
    "            torch.tensor: Output of the layer\n",
    "        \"\"\"\n",
    "        if not self.params_initialized:\n",
    "            self.initialize_params\n",
    "        \n",
    "        if not torch.is_tensor(x):\n",
    "            print(\n",
    "                f\"Found a non-tensor object: {type(x)}. Converting to tensor.\"\n",
    "            )\n",
    "            x = torch.tensor(x)\n",
    "\n",
    "        flattened_x = x.flatten()\n",
    "        # Get weighted sum + bias\n",
    "        weighted_sum = (\n",
    "            self.weights @ flattened_x.reshape(flattened_x.shape[0], 1)\n",
    "            + self.biases\n",
    "        )\n",
    "        if self.activation is None:\n",
    "            return weighted_sum\n",
    "        else:\n",
    "            return self.activation(weighted_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (521731785.py, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [4]\u001b[0;36m\u001b[0m\n\u001b[0;31m    layer.n_input_per_neuron =\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class ConvNet:\n",
    "    def __init__(self) -> None:\n",
    "        self.layers = []\n",
    "\n",
    "    def add_layer(self, layer) -> None:\n",
    "        \"\"\"Add Layers to ConvNet Model\n",
    "\n",
    "        Args:\n",
    "            layer (ConvLayer, Sequential): Layer to add to the model\n",
    "        \"\"\"\n",
    "        if len(self.layers) > 0:\n",
    "            last_layer = self.layers[-1]\n",
    "            # If last layer is convolutional layer\n",
    "            if isinstance(last_layer, ConvLayer):\n",
    "                input_dim_for_curr_layer = (\n",
    "                    len(last_layer.filters),\n",
    "                    last_layer.input_dim[1],\n",
    "                    last_layer.input_dim[2],\n",
    "                )\n",
    "                # If current layer is also conv layer set the correct input dim\n",
    "                if isinstance(layer, ConvLayer):\n",
    "                    layer.input_dim = input_dim_for_curr_layer\n",
    "\n",
    "                elif isinstance(layer, SequentialLayer):\n",
    "                    # Get the flatten layer dimension\n",
    "                    flattened_n = math.prod(\n",
    "                        input_dim_for_curr_layer\n",
    "                    )\n",
    "                    print(f\"flattened n: {flattened_n}\")\n",
    "                    layer.n_input_per_neuron = \n",
    "\n",
    "            # If last layer was convolutional\n",
    "            elif isinstance(last_layer, SequentialLayer) and isinstance(\n",
    "                layer, SequentialLayer\n",
    "            ):\n",
    "                # Set inputs of current sequential layer to the number of outputs of the last layer\n",
    "                layer.n_input_per_neuron = last_layer.neurons_per_layer\n",
    "\n",
    "        elif isinstance(layer, SequentialLayer):\n",
    "            print(\"Can't Start with a Sequential Layer.\")\n",
    "            return None\n",
    "\n",
    "        layer.initialize_params()\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, x_input: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"Do forward pass of the entire model\n",
    "\n",
    "        Args:\n",
    "            x_input (torch.tensor): Input Image\n",
    "\n",
    "        Returns:\n",
    "            torch.tensor: Output of the forward pass\n",
    "        \"\"\"\n",
    "        layer_input = x_input\n",
    "        last_layer_type = ConvLayer\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, SequentialLayer) and isinstance(layer, ConvLayer):\n",
    "                x_input = x_input.flatten()\n",
    "            layer_input = layer.forward_pass(layer_input)\n",
    "            last_layer_type = type(layer)\n",
    "        \n",
    "        return layer_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = MNIST(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor()\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_net = ConvNet()\n",
    "\n",
    "# Add Some Conv Layers\n",
    "conv_net.add_layer(ConvLayer(input_dim=(1, 28, 28), filter_size=3, n_filters=16))\n",
    "conv_net.add_layer(ConvLayer(filter_size=3, n_filters=32))\n",
    "conv_net.add_layer(ConvLayer(filter_size=3, n_filters=64))\n",
    "conv_net.add_layer(ConvLayer(filter_size=3, n_filters=128))\n",
    "conv_net.add_layer(SequentialLayer(100, activation=torch.relu))\n",
    "# conv_net.add_layer(SequentialLayer(10, activation=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(100, 0), requires_grad=True)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_net.layers[-1].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (100x0 and 100352x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/harshagrawal/Downloads/code/machine_learning/cnn-torch-scratch/torch-cnn.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/harshagrawal/Downloads/code/machine_learning/cnn-torch-scratch/torch-cnn.ipynb#Y105sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m x \u001b[39m=\u001b[39m conv_net\u001b[39m.\u001b[39;49mforward(mnist_trainset[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m])\n",
      "\u001b[1;32m/Users/harshagrawal/Downloads/code/machine_learning/cnn-torch-scratch/torch-cnn.ipynb Cell 8\u001b[0m in \u001b[0;36mConvNet.forward\u001b[0;34m(self, x_input)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshagrawal/Downloads/code/machine_learning/cnn-torch-scratch/torch-cnn.ipynb#Y105sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(layer, SequentialLayer) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(layer, ConvLayer):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshagrawal/Downloads/code/machine_learning/cnn-torch-scratch/torch-cnn.ipynb#Y105sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m         x_input \u001b[39m=\u001b[39m x_input\u001b[39m.\u001b[39mflatten()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/harshagrawal/Downloads/code/machine_learning/cnn-torch-scratch/torch-cnn.ipynb#Y105sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     layer_input \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mforward_pass(layer_input)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshagrawal/Downloads/code/machine_learning/cnn-torch-scratch/torch-cnn.ipynb#Y105sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     last_layer_type \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(layer)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshagrawal/Downloads/code/machine_learning/cnn-torch-scratch/torch-cnn.ipynb#Y105sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mreturn\u001b[39;00m layer_input\n",
      "\u001b[1;32m/Users/harshagrawal/Downloads/code/machine_learning/cnn-torch-scratch/torch-cnn.ipynb Cell 8\u001b[0m in \u001b[0;36mSequentialLayer.forward_pass\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshagrawal/Downloads/code/machine_learning/cnn-torch-scratch/torch-cnn.ipynb#Y105sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m flattened_x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshagrawal/Downloads/code/machine_learning/cnn-torch-scratch/torch-cnn.ipynb#Y105sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39m# Get weighted sum + bias\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshagrawal/Downloads/code/machine_learning/cnn-torch-scratch/torch-cnn.ipynb#Y105sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m weighted_sum \u001b[39m=\u001b[39m (\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/harshagrawal/Downloads/code/machine_learning/cnn-torch-scratch/torch-cnn.ipynb#Y105sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights \u001b[39m@\u001b[39;49m flattened_x\u001b[39m.\u001b[39;49mreshape(flattened_x\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], \u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshagrawal/Downloads/code/machine_learning/cnn-torch-scratch/torch-cnn.ipynb#Y105sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbiases\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshagrawal/Downloads/code/machine_learning/cnn-torch-scratch/torch-cnn.ipynb#Y105sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshagrawal/Downloads/code/machine_learning/cnn-torch-scratch/torch-cnn.ipynb#Y105sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harshagrawal/Downloads/code/machine_learning/cnn-torch-scratch/torch-cnn.ipynb#Y105sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m weighted_sum\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (100x0 and 100352x1)"
     ]
    }
   ],
   "source": [
    "x = conv_net.forward(mnist_trainset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 28, 28])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = ConvLayer(filter_size=3, n_filters=32, input_dim=(16, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set Image Channels: 16; Img Size: 30\n",
      "Curr Image Channels: 16; Img Size: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 8.9977,  6.4394,  6.4394,  ...,  6.4394,  6.4394,  0.8392],\n",
       "         [ 7.4137,  5.5592,  5.5592,  ...,  5.5592,  5.5592,  0.0000],\n",
       "         [ 7.4137,  5.5592,  5.5592,  ...,  5.5592,  5.5592,  0.0000],\n",
       "         ...,\n",
       "         [ 7.4137,  5.5592,  5.4245,  ...,  5.5592,  5.5592,  0.0000],\n",
       "         [ 7.4137,  5.5592,  4.2788,  ...,  5.5592,  5.5592,  0.0000],\n",
       "         [ 1.1926,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 3.3620,  5.3266,  5.3266,  ...,  5.3266,  5.3266,  0.8295],\n",
       "         [ 8.6206,  7.6284,  7.6284,  ...,  7.6284,  7.6284,  0.2067],\n",
       "         [ 8.6206,  7.6284,  7.6284,  ...,  7.6284,  7.6284,  0.2067],\n",
       "         ...,\n",
       "         [ 8.6206,  7.6284,  5.7642,  ...,  7.6284,  7.6284,  0.2067],\n",
       "         [ 8.6206,  7.6284,  7.7404,  ...,  7.6284,  7.6284,  0.2067],\n",
       "         [10.9487,  9.7599,  9.7599,  ...,  9.7599,  9.7599,  4.1060]],\n",
       "\n",
       "        [[10.1675,  5.4927,  5.4927,  ...,  5.4927,  5.4927,  2.9021],\n",
       "         [10.0439,  4.4333,  4.4333,  ...,  4.4333,  4.4333,  3.2166],\n",
       "         [10.0439,  4.4333,  4.4333,  ...,  4.4333,  4.4333,  3.2166],\n",
       "         ...,\n",
       "         [10.0439,  4.4333,  6.6541,  ...,  4.4333,  4.4333,  3.2166],\n",
       "         [10.0439,  4.4333,  6.9475,  ...,  4.4333,  4.4333,  3.2166],\n",
       "         [ 9.3096,  6.7126,  6.7126,  ...,  6.7126,  6.7126,  7.2049]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 8.1898,  4.1483,  4.1483,  ...,  4.1483,  4.1483,  0.0000],\n",
       "         [ 6.5198,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 6.5198,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 6.5198,  0.0000,  0.1288,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 6.5198,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.1608,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  3.7179,  3.7179,  ...,  3.7179,  3.7179,  4.9154]]],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.forward_pass(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True),\n",
       " tensor([], size=(0, 3, 3), requires_grad=True)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_net.layers[1].filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('test-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "780f284e0ebdf609724e48d6015990f26a433b2d11fcba8f8b08df791d1e6f9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
