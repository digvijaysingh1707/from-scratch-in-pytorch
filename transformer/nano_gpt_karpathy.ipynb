{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This repository is based on Andrej Karpathy's implmentation of Nano-GPT. Karpathy = Chad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "with open(\"shakespear.txt\", \"r\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "# defining vocab\n",
    "vocab = sorted(list(set(data)))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# lookup dicts\n",
    "ix_to_char = {i: ch for i, ch in enumerate(vocab)}\n",
    "char_to_ix = {ch: i for i, ch in enumerate(vocab)}\n",
    "\n",
    "def encode(input_str: str) -> torch.tensor:\n",
    "    encoded = []\n",
    "    for ch in input_str:\n",
    "        encoded.append(char_to_ix[ch])\n",
    "    return torch.tensor(encoded)\n",
    "\n",
    "def decode(input_ints: torch.tensor) -> str:\n",
    "    decoded = \"\"\n",
    "    for ix in input_ints:\n",
    "        decoded += ix_to_char[ix.item()]\n",
    "    return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode all data & split into train and val\n",
    "data_encoded = encode(data)\n",
    "\n",
    "train_split = 0.8\n",
    "split_idx = int(train_split * len(data_encoded))\n",
    "train_data = data_encoded[:split_idx]\n",
    "eval_data = data_encoded[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  # B\n",
    "block_size = 16  # T\n",
    "\n",
    "def generate_batch(split_type):\n",
    "    if split_type == \"train\":\n",
    "        batch_to_generate_from = train_data\n",
    "    else:\n",
    "        batch_to_generate_from = eval_data\n",
    "\n",
    "    # generate random starting points\n",
    "    ixes = torch.randint(0, len(batch_to_generate_from) - block_size, (batch_size,))\n",
    "\n",
    "    # extend the generated starting points\n",
    "    X = torch.stack([batch_to_generate_from[ix:ix+block_size] for ix in ixes]) # B, T\n",
    "    Y = torch.stack([batch_to_generate_from[ix+1:ix+block_size+1] for ix in ixes]) # B, T\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = generate_batch(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_size = 32\n",
    "emb_size = 64\n",
    "\n",
    "class SingleHead(nn.Module):\n",
    "    def __init__(self, head_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.l_key = nn.Linear(emb_size, head_size)\n",
    "        self.l_query = nn.Linear(emb_size, head_size)\n",
    "        self.l_value = nn.Linear(emb_size, head_size)\n",
    "        self.ff = nn.Linear(head_size, emb_size)\n",
    "\n",
    "    def forward(self, X) -> torch.tensor:\n",
    "        \"\"\"Forward Function\n",
    "\n",
    "        Args:\n",
    "            X (torch.tensor): X should be the output of sem_emb + pos_emb of shape B, T, emb_size\n",
    "\n",
    "        Returns:\n",
    "            torch.tensor: _description_\n",
    "        \"\"\"\n",
    "        Q = self.l_query(X) # B, T, head_size\n",
    "        K = self.l_key(X) # B, T, head_size\n",
    "        V = self.l_value(X) # B, T, head_size\n",
    "        # Produce weights\n",
    "        wei = Q @ K.transpose(-1, -2) # B, T, T\n",
    "        tril = torch.tril(torch.ones(block_size, block_size))\n",
    "        masked_wei = wei.masked_fill(tril==0, float('-inf')) / (head_size ** 0.5)\n",
    "        soft_wei = masked_wei.softmax(-1) # B, T, T\n",
    "\n",
    "        out = soft_wei @ V # B, T, head_size\n",
    "        return out\n",
    "        # return self.ff(out) # B, T, emb_size\n",
    "        \n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.attention_blocks = nn.ModuleList([SingleHead(head_size//n_heads) for i in range(n_heads)])\n",
    "        self.proj_layer = nn.Linear(head_size, emb_size)\n",
    "    def forward(self, X) -> torch.tensor:\n",
    "        out = torch.cat([self.attention_blocks[ix](X) for ix in range(self.n_heads)], -1)\n",
    "        return self.proj_layer(out)  # 4, 8, 16 -> 4, 8, 32\n",
    "\n",
    "class FeedFowardLayer(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.feed_foward = nn.Sequential(\n",
    "            nn.Linear(emb_size, emb_size * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_size * 4, emb_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            X (_type_): Should be the output of MHA. Output shape: B, T, head_size\n",
    "\n",
    "        Returns:\n",
    "            torch.tensor: Output shape: B, T, emb_size\n",
    "        \"\"\"\n",
    "        return self.feed_foward(X) # B, T, emb_size\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, n_heads: int) -> None:\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadedAttention(n_heads)\n",
    "        self.ff = FeedFowardLayer()\n",
    "        self.layer_norm = nn.LayerNorm(emb_size)\n",
    "\n",
    "    def forward(self, X: torch.tensor):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            X (torch.tensor): Should be input emb (sem_emb + pos_emb)\n",
    "        \"\"\"\n",
    "        X = self.mha(self.layer_norm(X)) + X\n",
    "        out = self.ff(self.layer_norm(X)) + X\n",
    "        return out\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, num_blocks, n_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.semantic_embedding_table = nn.Embedding(vocab_size, emb_size)\n",
    "        self.positional_emb_table = nn.Embedding(block_size, emb_size)\n",
    "        self.attention_layers = nn.Sequential(\n",
    "            *[AttentionBlock(n_heads) for i in range(num_blocks)]\n",
    "        )\n",
    "        self.linear_layer = nn.Linear(emb_size, vocab_size)    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        sem_emb = self.semantic_embedding_table(X) # B, T, emb_size\n",
    "        # TODO: Check if position start from 0 or 1\n",
    "        pos_emb = self.positional_emb_table(torch.arange(block_size)) # T, emb_size\n",
    "        att_out = self.attention_layers(sem_emb + pos_emb) # B, T, emb_size\n",
    "        return self.linear_layer(att_out) # B, T, vocab_size\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        opt = torch.optim.AdamW(self.parameters())\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            X, Y = generate_batch(\"train\")\n",
    "            out = self.forward(X)\n",
    "            logits = out.softmax(-1)\n",
    "            loss = loss_func(logits.view(batch_size * block_size, vocab_size), Y.flatten())\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT(5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.206117630004883\n",
      "Loss: 4.201798915863037\n",
      "Loss: 4.192501068115234\n",
      "Loss: 4.180886268615723\n",
      "Loss: 4.177687644958496\n",
      "Loss: 4.144900798797607\n",
      "Loss: 4.128637313842773\n",
      "Loss: 4.116065502166748\n",
      "Loss: 4.083380222320557\n",
      "Loss: 4.064646244049072\n"
     ]
    }
   ],
   "source": [
    "gpt.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c741f2e161574c3cb93bc7475ad46271fa01a2bf647187abe1b406a686238bfc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
