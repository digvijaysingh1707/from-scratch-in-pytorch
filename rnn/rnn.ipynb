{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "dataset_raw = open(\"data/shakespear.txt\").read()\n",
    "# Create x & y raw (non-encoded)\n",
    "x_raw = list(dataset_raw)\n",
    "y_raw = [x for x in x_raw[1:]]\n",
    "y_raw.append(\" \") # to make it the same size as x_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \n",
    "idx_to_char = {idx: char for idx, char in enumerate(set(x_raw))}\n",
    "char_to_idx = {char: idx for idx, char in idx_to_char.items()}\n",
    "\n",
    "vocab_size = len(idx_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode X & Y\n",
    "x = torch.zeros(len(x_raw), vocab_size, dtype=torch.float32)\n",
    "y = torch.zeros(len(y_raw), vocab_size)\n",
    "\n",
    "for i in range(len(x_raw)):\n",
    "    x[i][char_to_idx[x_raw[i]]] = 1\n",
    "\n",
    "for i in range(len(y_raw)):\n",
    "    y[i][char_to_idx[y_raw[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLayer:\n",
    "    def __init__(self, input_dim: int, hidden_state_size: int) -> None:\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_state_size = hidden_state_size\n",
    "\n",
    "        # Definine H0 (initial hidden state)\n",
    "        self.h = torch.randn(100, dtype=torch.float32)\n",
    "\n",
    "        # Initialize Weights\n",
    "        self.w_xh = torch.randn(hidden_state_size, input_dim, dtype=torch.float32, requires_grad=True)\n",
    "        self.w_hh = torch.randn(hidden_state_size, hidden_state_size, dtype=torch.float32, requires_grad=True)\n",
    "        self.w_hy = torch.randn(input_dim, hidden_state_size, dtype=torch.float32, requires_grad=True)\n",
    "        \n",
    "        # Initialize Biases\n",
    "        # self.b_h = torch.randn(hidden_state_size, dtype=torch.float32, requires_grad=True)\n",
    "        # self.b_y = torch.randn(input_dim, dtype=torch.float32, requires_grad=True)\n",
    "        self.model_params = [self.w_xh, self.w_hy, self.w_hh]\n",
    "\n",
    "    def train(self, x: torch.tensor, y: torch.tensor, timesteps: int, epochs: int, lr: float) -> torch.tensor:\n",
    "        \"\"\"Forward Function\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): Input Vector\n",
    "            timesteps (int): Timestep to do the RNN\n",
    "\n",
    "        Returns:\n",
    "            torch.tensor: Output of the forward pass\n",
    "        \"\"\"\n",
    "        smooth_loss = torch.tensor(-np.log(1.0/67)*100)\n",
    "        for epoch in range(epochs):\n",
    "            # start_index = random.randint(0, len(x))\n",
    "            start_index = 0\n",
    "            \n",
    "            loss_func = CrossEntropyLoss()\n",
    "            loss = torch.zeros(1, dtype=torch.float32)\n",
    "            \n",
    "            for timestep in range(start_index, timesteps):\n",
    "                # Update the hidden state\n",
    "                self.h = torch.tanh(self.w_xh @ x[timestep] + self.w_hh @ self.h)\n",
    "                y_hat = (self.w_hy @ self.h).softmax(0)\n",
    "                loss += loss_func(y_hat, y[timestep])\n",
    "            \n",
    "            complete_loss = 0.99 * smooth_loss + 0.01 * loss\n",
    "            print(f\"Epoch: {epoch}; Loss: {loss.item()}\")\n",
    "\n",
    "            for param in self.model_params:\n",
    "                param.grad = None\n",
    "            \n",
    "            complete_loss.backward(retain_graph=True)\n",
    "\n",
    "            for param in self.model_params:\n",
    "                # param.data -= lr * np.clip(param.grad, -10, 10)\n",
    "                param.data -= lr * param.grad\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNNLayer(67, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 420.3501892089844\n",
      "Epoch: 1; Loss: 422.21624755859375\n",
      "Epoch: 2; Loss: 421.87255859375\n",
      "Epoch: 3; Loss: 420.741455078125\n",
      "Epoch: 4; Loss: 421.73828125\n",
      "Epoch: 5; Loss: 421.8819580078125\n",
      "Epoch: 6; Loss: 421.7661437988281\n",
      "Epoch: 7; Loss: 422.5824279785156\n",
      "Epoch: 8; Loss: 420.39697265625\n",
      "Epoch: 9; Loss: 420.6890869140625\n"
     ]
    }
   ],
   "source": [
    "y_hat = rnn.train(x, y, 100, 10, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('test-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "780f284e0ebdf609724e48d6015990f26a433b2d11fcba8f8b08df791d1e6f9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
