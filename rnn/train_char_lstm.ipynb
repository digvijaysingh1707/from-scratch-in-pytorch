{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cpu\n"
     ]
    }
   ],
   "source": [
    "from char_lstm_torch import SequentialDataGenerator, CharLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = SequentialDataGenerator(\"data/dante.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyper paramters\n",
    "num_layers = 2\n",
    "dropout_probab = 0.4\n",
    "hidden_size = 512\n",
    "\n",
    "seq_len = 100\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = CharLSTM(\n",
    "    input_size=datagen.vocab_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout_probab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 841792; Vocab Size: 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  17%|█▋        | 13/75 [00:10<00:52,  1.19 batch/s, loss=3.22, val_loss=0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/harshagrawal/Downloads/code/machine_learning/from-scratch-in-pytorch/rnn/lstm_torch.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/harshagrawal/Downloads/code/machine_learning/from-scratch-in-pytorch/rnn/lstm_torch.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m lstm\u001b[39m.\u001b[39;49mtrain_model(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshagrawal/Downloads/code/machine_learning/from-scratch-in-pytorch/rnn/lstm_torch.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     datagen\u001b[39m=\u001b[39;49mdatagen,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshagrawal/Downloads/code/machine_learning/from-scratch-in-pytorch/rnn/lstm_torch.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshagrawal/Downloads/code/machine_learning/from-scratch-in-pytorch/rnn/lstm_torch.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshagrawal/Downloads/code/machine_learning/from-scratch-in-pytorch/rnn/lstm_torch.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     seq_length\u001b[39m=\u001b[39;49mseq_len,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshagrawal/Downloads/code/machine_learning/from-scratch-in-pytorch/rnn/lstm_torch.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdante\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/harshagrawal/Downloads/code/machine_learning/from-scratch-in-pytorch/rnn/lstm_torch.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Downloads/code/machine_learning/from-scratch-in-pytorch/rnn/char_lstm_torch.py:278\u001b[0m, in \u001b[0;36mCharLSTM.train_model\u001b[0;34m(self, datagen, epochs, batch_size, seq_length, lr, clip, val_frac, save_model, model_name)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39m# calculate the loss and perform backprop\u001b[39;00m\n\u001b[1;32m    275\u001b[0m loss \u001b[39m=\u001b[39m loss_function(\n\u001b[1;32m    276\u001b[0m     output, y\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(batch_size \u001b[39m*\u001b[39m seq_length)\n\u001b[1;32m    277\u001b[0m )\n\u001b[0;32m--> 278\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    280\u001b[0m \u001b[39m# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\u001b[39;00m\n\u001b[1;32m    281\u001b[0m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameters(), clip)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/test-env/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/test-env/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm.train_model(\n",
    "    datagen=datagen,\n",
    "    epochs=10,\n",
    "    batch_size=batch_size,\n",
    "    seq_length=seq_len,\n",
    "    model_name=\"dante\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.load_model(\"saved_models/dante/epoch_10.net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You shall in Molilen diurs, on goies ligy and the at his\\nlead turn\\nsporpuitlis, thene who shine oud berived, sould, and and I bosk as de dithine who hom?[4] Af ad I saw natwern of hyat Pradebeom Bea Ged albers so lark, who crrued, ol ye anow than far hord maked the criit of the moves tomis wond to long baskod my good be.\\n\\n\\nBe fird that the worming spoble of mise God with distunes\\njoss and has bores was\\nme, '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.sample(\n",
    "    datagen,\n",
    "    400,\n",
    "    \"You shall\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('test-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "780f284e0ebdf609724e48d6015990f26a433b2d11fcba8f8b08df791d1e6f9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
