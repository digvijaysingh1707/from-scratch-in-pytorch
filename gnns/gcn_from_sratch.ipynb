{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a simple 2-layered GCN from scratch in Pytorch\n",
    "Associated Paper: https://arxiv.org/pdf/1609.02907.pdf\n",
    "Example Taken: 2-layered GCN on CORA Dataset\n",
    "\n",
    "_TODO: Modularize GCN class_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/cora/\"\n",
    "edgelist = pd.read_csv(os.path.join(data_dir, \"cora.cites\"), sep='\\t', header=None, names=[\"target\", \"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers shape: (2708, 1435)\n"
     ]
    }
   ],
   "source": [
    "column_names = [\"paper_id\"] + [f\"term_{idx}\" for idx in range(1433)] + [\"subject\"]\n",
    "papers = pd.read_csv(\n",
    "    os.path.join(data_dir, \"cora.content\"), sep=\"\\t\", header=None, names=column_names,\n",
    ")\n",
    "print(\"Papers shape:\", papers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper Index -> Id & Vice Verca\n",
    "paper_idx_to_id = {idx_: id_ for idx_, id_ in enumerate(list(set(papers[\"paper_id\"])))}\n",
    "\n",
    "# Paper Id -> Index\n",
    "paper_id_to_idx = {id_: idx_ for idx_, id_ in paper_idx_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Normalized Adjacency Matrix -> A_HAT\n",
    "\n",
    "# Create an empty Adjacency matrix\n",
    "A = torch.zeros(papers.shape[0], papers.shape[0])\n",
    "\n",
    "# Fill the adjacency matrix wherever there is an edge\n",
    "for pair in edgelist.values:\n",
    "    A[paper_id_to_idx[pair[0]], paper_id_to_idx[pair[1]]] = 1\n",
    "\n",
    "# Create an Identity Matrix for A\n",
    "I = torch.eye(A.shape[0])\n",
    "\n",
    "# A_TILDA = A + I\n",
    "A_TILDA = A + I\n",
    "\n",
    "# Create the Inverse Squared Diagonal Matrix of A_TILDA\n",
    "D_TILDA_INVERSE_SQUARED = torch.zeros_like(A_TILDA)\n",
    "\n",
    "for i in range(len(A_TILDA)):\n",
    "    D_TILDA_INVERSE_SQUARED[i, i] = A_TILDA[i].sum().pow(-0.5)\n",
    "\n",
    "# Finally A_HAT = D_TILDA_INVERSE_SQUARED @ A_TILDA @ D_TILDA_INVERSE_SQUARED\n",
    "A_HAT = D_TILDA_INVERSE_SQUARED @ A_TILDA @ D_TILDA_INVERSE_SQUARED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 2708])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a two layered GCN model\n",
    "A_HAT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE X & Y: Preparing Dataset\n",
    "dataset = papers.values\n",
    "pairwise_indices = [x[0] for x in dataset]\n",
    "X_PRE = []\n",
    "Y_PRE = []\n",
    "for i in range(len(dataset)):\n",
    "    idx_ = pairwise_indices.index(paper_idx_to_id[i])    \n",
    "    X_PRE.append(list(dataset[idx_][1:-1]))\n",
    "    Y_PRE.append(dataset[idx_][-1])\n",
    "\n",
    "X = torch.tensor(X_PRE, dtype=torch.float32)\n",
    "\n",
    "# Convert Y to Onehot\n",
    "\n",
    "y_idx_to_label = {idx_: label_ for idx_, label_ in enumerate(set(Y_PRE))}\n",
    "y_label_to_idx = {label_: idx_ for idx_, label_ in y_idx_to_label.items()}\n",
    "\n",
    "Y_ONE_HOT = torch.zeros(len(Y_PRE), len(set(Y_PRE)))\n",
    "for idx_ in range(len(Y_PRE)):\n",
    "    row = torch.zeros(7)\n",
    "    row[y_label_to_idx[Y_PRE[idx_]]] = 1\n",
    "    Y_ONE_HOT[idx_] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model Weights\n",
    "feature_vector_size = X.shape[1]\n",
    "hidden_layer_size = 100\n",
    "output_size = len(set(Y_PRE))\n",
    "\n",
    "# Weights at layer 0, 1\n",
    "W_0 = torch.randn(feature_vector_size, hidden_layer_size, dtype=torch.float32, requires_grad=True)\n",
    "W_1 = torch.randn(hidden_layer_size, output_size, dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Model Loss: 36.43867874145508\n",
      "Epoch: 1; Model Loss: 34.91018295288086\n",
      "Epoch: 2; Model Loss: 33.516510009765625\n",
      "Epoch: 3; Model Loss: 32.23931884765625\n",
      "Epoch: 4; Model Loss: 31.07048797607422\n",
      "Epoch: 5; Model Loss: 29.99709701538086\n",
      "Epoch: 6; Model Loss: 29.00690460205078\n",
      "Epoch: 7; Model Loss: 28.082691192626953\n",
      "Epoch: 8; Model Loss: 27.219947814941406\n",
      "Epoch: 9; Model Loss: 26.417091369628906\n",
      "Epoch: 10; Model Loss: 25.66851043701172\n",
      "Epoch: 11; Model Loss: 24.967445373535156\n",
      "Epoch: 12; Model Loss: 24.306983947753906\n",
      "Epoch: 13; Model Loss: 23.686738967895508\n",
      "Epoch: 14; Model Loss: 23.102771759033203\n",
      "Epoch: 15; Model Loss: 22.555030822753906\n",
      "Epoch: 16; Model Loss: 22.033605575561523\n",
      "Epoch: 17; Model Loss: 21.530256271362305\n",
      "Epoch: 18; Model Loss: 21.041397094726562\n",
      "Epoch: 19; Model Loss: 20.568418502807617\n",
      "Epoch: 20; Model Loss: 20.11102867126465\n",
      "Epoch: 21; Model Loss: 19.671627044677734\n",
      "Epoch: 22; Model Loss: 19.24673080444336\n",
      "Epoch: 23; Model Loss: 18.8330135345459\n",
      "Epoch: 24; Model Loss: 18.42973518371582\n",
      "Epoch: 25; Model Loss: 18.03466033935547\n",
      "Epoch: 26; Model Loss: 17.648584365844727\n",
      "Epoch: 27; Model Loss: 17.272846221923828\n",
      "Epoch: 28; Model Loss: 16.9028263092041\n",
      "Epoch: 29; Model Loss: 16.53949546813965\n",
      "Epoch: 30; Model Loss: 16.18817710876465\n",
      "Epoch: 31; Model Loss: 15.849907875061035\n",
      "Epoch: 32; Model Loss: 15.523782730102539\n",
      "Epoch: 33; Model Loss: 15.206440925598145\n",
      "Epoch: 34; Model Loss: 14.897576332092285\n",
      "Epoch: 35; Model Loss: 14.598959922790527\n",
      "Epoch: 36; Model Loss: 14.304909706115723\n",
      "Epoch: 37; Model Loss: 14.014569282531738\n",
      "Epoch: 38; Model Loss: 13.729413032531738\n",
      "Epoch: 39; Model Loss: 13.451000213623047\n",
      "Epoch: 40; Model Loss: 13.178437232971191\n",
      "Epoch: 41; Model Loss: 12.912692070007324\n",
      "Epoch: 42; Model Loss: 12.65545654296875\n",
      "Epoch: 43; Model Loss: 12.406623840332031\n",
      "Epoch: 44; Model Loss: 12.166536331176758\n",
      "Epoch: 45; Model Loss: 11.93440055847168\n",
      "Epoch: 46; Model Loss: 11.710814476013184\n",
      "Epoch: 47; Model Loss: 11.49478816986084\n",
      "Epoch: 48; Model Loss: 11.286470413208008\n",
      "Epoch: 49; Model Loss: 11.08370590209961\n",
      "Epoch: 50; Model Loss: 10.887160301208496\n",
      "Epoch: 51; Model Loss: 10.694652557373047\n",
      "Epoch: 52; Model Loss: 10.507905960083008\n",
      "Epoch: 53; Model Loss: 10.326760292053223\n",
      "Epoch: 54; Model Loss: 10.150938034057617\n",
      "Epoch: 55; Model Loss: 9.979888916015625\n",
      "Epoch: 56; Model Loss: 9.814355850219727\n",
      "Epoch: 57; Model Loss: 9.653180122375488\n",
      "Epoch: 58; Model Loss: 9.497071266174316\n",
      "Epoch: 59; Model Loss: 9.345412254333496\n",
      "Epoch: 60; Model Loss: 9.19847583770752\n",
      "Epoch: 61; Model Loss: 9.05661392211914\n",
      "Epoch: 62; Model Loss: 8.920073509216309\n",
      "Epoch: 63; Model Loss: 8.787895202636719\n",
      "Epoch: 64; Model Loss: 8.659477233886719\n",
      "Epoch: 65; Model Loss: 8.534781455993652\n",
      "Epoch: 66; Model Loss: 8.413850784301758\n",
      "Epoch: 67; Model Loss: 8.295979499816895\n",
      "Epoch: 68; Model Loss: 8.181242942810059\n",
      "Epoch: 69; Model Loss: 8.069942474365234\n",
      "Epoch: 70; Model Loss: 7.961940765380859\n",
      "Epoch: 71; Model Loss: 7.856141090393066\n",
      "Epoch: 72; Model Loss: 7.752926826477051\n",
      "Epoch: 73; Model Loss: 7.652719020843506\n",
      "Epoch: 74; Model Loss: 7.5552239418029785\n",
      "Epoch: 75; Model Loss: 7.459195613861084\n",
      "Epoch: 76; Model Loss: 7.365879058837891\n",
      "Epoch: 77; Model Loss: 7.275264263153076\n",
      "Epoch: 78; Model Loss: 7.186661243438721\n",
      "Epoch: 79; Model Loss: 7.099877834320068\n",
      "Epoch: 80; Model Loss: 7.014752388000488\n",
      "Epoch: 81; Model Loss: 6.931605815887451\n",
      "Epoch: 82; Model Loss: 6.850190162658691\n",
      "Epoch: 83; Model Loss: 6.7708539962768555\n",
      "Epoch: 84; Model Loss: 6.693490505218506\n",
      "Epoch: 85; Model Loss: 6.6179118156433105\n",
      "Epoch: 86; Model Loss: 6.543864727020264\n",
      "Epoch: 87; Model Loss: 6.4714674949646\n",
      "Epoch: 88; Model Loss: 6.400801181793213\n",
      "Epoch: 89; Model Loss: 6.3319621086120605\n",
      "Epoch: 90; Model Loss: 6.264458179473877\n",
      "Epoch: 91; Model Loss: 6.198418140411377\n",
      "Epoch: 92; Model Loss: 6.133706092834473\n",
      "Epoch: 93; Model Loss: 6.070712089538574\n",
      "Epoch: 94; Model Loss: 6.009215354919434\n",
      "Epoch: 95; Model Loss: 5.94894552230835\n",
      "Epoch: 96; Model Loss: 5.889715194702148\n",
      "Epoch: 97; Model Loss: 5.83189582824707\n",
      "Epoch: 98; Model Loss: 5.7753424644470215\n",
      "Epoch: 99; Model Loss: 5.7198944091796875\n",
      "Epoch: 100; Model Loss: 5.665430068969727\n",
      "Epoch: 101; Model Loss: 5.611770153045654\n",
      "Epoch: 102; Model Loss: 5.558934211730957\n",
      "Epoch: 103; Model Loss: 5.506973743438721\n",
      "Epoch: 104; Model Loss: 5.456188678741455\n",
      "Epoch: 105; Model Loss: 5.406467437744141\n",
      "Epoch: 106; Model Loss: 5.3573737144470215\n",
      "Epoch: 107; Model Loss: 5.308592319488525\n",
      "Epoch: 108; Model Loss: 5.260879039764404\n",
      "Epoch: 109; Model Loss: 5.214036464691162\n",
      "Epoch: 110; Model Loss: 5.168204307556152\n",
      "Epoch: 111; Model Loss: 5.1232075691223145\n",
      "Epoch: 112; Model Loss: 5.079361438751221\n",
      "Epoch: 113; Model Loss: 5.036410331726074\n",
      "Epoch: 114; Model Loss: 4.994219779968262\n",
      "Epoch: 115; Model Loss: 4.952887058258057\n",
      "Epoch: 116; Model Loss: 4.911979675292969\n",
      "Epoch: 117; Model Loss: 4.871763706207275\n",
      "Epoch: 118; Model Loss: 4.832249164581299\n",
      "Epoch: 119; Model Loss: 4.793607711791992\n",
      "Epoch: 120; Model Loss: 4.7558746337890625\n",
      "Epoch: 121; Model Loss: 4.718986511230469\n",
      "Epoch: 122; Model Loss: 4.68277645111084\n",
      "Epoch: 123; Model Loss: 4.647263526916504\n",
      "Epoch: 124; Model Loss: 4.612361431121826\n",
      "Epoch: 125; Model Loss: 4.578200340270996\n",
      "Epoch: 126; Model Loss: 4.544823169708252\n",
      "Epoch: 127; Model Loss: 4.512177467346191\n",
      "Epoch: 128; Model Loss: 4.480205059051514\n",
      "Epoch: 129; Model Loss: 4.448770999908447\n",
      "Epoch: 130; Model Loss: 4.417965888977051\n",
      "Epoch: 131; Model Loss: 4.387818813323975\n",
      "Epoch: 132; Model Loss: 4.35822057723999\n",
      "Epoch: 133; Model Loss: 4.329024314880371\n",
      "Epoch: 134; Model Loss: 4.300387859344482\n",
      "Epoch: 135; Model Loss: 4.272189617156982\n",
      "Epoch: 136; Model Loss: 4.2446184158325195\n",
      "Epoch: 137; Model Loss: 4.217589378356934\n",
      "Epoch: 138; Model Loss: 4.191080093383789\n",
      "Epoch: 139; Model Loss: 4.165066242218018\n",
      "Epoch: 140; Model Loss: 4.13913106918335\n",
      "Epoch: 141; Model Loss: 4.11365270614624\n",
      "Epoch: 142; Model Loss: 4.088529586791992\n",
      "Epoch: 143; Model Loss: 4.063460826873779\n",
      "Epoch: 144; Model Loss: 4.038638114929199\n",
      "Epoch: 145; Model Loss: 4.014366626739502\n",
      "Epoch: 146; Model Loss: 3.990403413772583\n",
      "Epoch: 147; Model Loss: 3.9669840335845947\n",
      "Epoch: 148; Model Loss: 3.944002389907837\n",
      "Epoch: 149; Model Loss: 3.9213902950286865\n",
      "Epoch: 150; Model Loss: 3.899156332015991\n",
      "Epoch: 151; Model Loss: 3.8773584365844727\n",
      "Epoch: 152; Model Loss: 3.8559529781341553\n",
      "Epoch: 153; Model Loss: 3.8350064754486084\n",
      "Epoch: 154; Model Loss: 3.814451217651367\n",
      "Epoch: 155; Model Loss: 3.794220209121704\n",
      "Epoch: 156; Model Loss: 3.7743146419525146\n",
      "Epoch: 157; Model Loss: 3.7547879219055176\n",
      "Epoch: 158; Model Loss: 3.7357068061828613\n",
      "Epoch: 159; Model Loss: 3.7169320583343506\n",
      "Epoch: 160; Model Loss: 3.6984808444976807\n",
      "Epoch: 161; Model Loss: 3.680412769317627\n",
      "Epoch: 162; Model Loss: 3.662655830383301\n",
      "Epoch: 163; Model Loss: 3.6452012062072754\n",
      "Epoch: 164; Model Loss: 3.628009796142578\n",
      "Epoch: 165; Model Loss: 3.6112120151519775\n",
      "Epoch: 166; Model Loss: 3.5947728157043457\n",
      "Epoch: 167; Model Loss: 3.5786709785461426\n",
      "Epoch: 168; Model Loss: 3.5627870559692383\n",
      "Epoch: 169; Model Loss: 3.5471150875091553\n",
      "Epoch: 170; Model Loss: 3.5317327976226807\n",
      "Epoch: 171; Model Loss: 3.516589403152466\n",
      "Epoch: 172; Model Loss: 3.5017385482788086\n",
      "Epoch: 173; Model Loss: 3.487102746963501\n",
      "Epoch: 174; Model Loss: 3.4726293087005615\n",
      "Epoch: 175; Model Loss: 3.4583914279937744\n",
      "Epoch: 176; Model Loss: 3.4443676471710205\n",
      "Epoch: 177; Model Loss: 3.4305903911590576\n",
      "Epoch: 178; Model Loss: 3.4169821739196777\n",
      "Epoch: 179; Model Loss: 3.403566360473633\n",
      "Epoch: 180; Model Loss: 3.3902769088745117\n",
      "Epoch: 181; Model Loss: 3.3771960735321045\n",
      "Epoch: 182; Model Loss: 3.3642923831939697\n",
      "Epoch: 183; Model Loss: 3.351656913757324\n",
      "Epoch: 184; Model Loss: 3.3392395973205566\n",
      "Epoch: 185; Model Loss: 3.3270280361175537\n",
      "Epoch: 186; Model Loss: 3.3150548934936523\n",
      "Epoch: 187; Model Loss: 3.30330491065979\n",
      "Epoch: 188; Model Loss: 3.291717767715454\n",
      "Epoch: 189; Model Loss: 3.280324935913086\n",
      "Epoch: 190; Model Loss: 3.2690839767456055\n",
      "Epoch: 191; Model Loss: 3.2580137252807617\n",
      "Epoch: 192; Model Loss: 3.247129440307617\n",
      "Epoch: 193; Model Loss: 3.2364630699157715\n",
      "Epoch: 194; Model Loss: 3.2259790897369385\n",
      "Epoch: 195; Model Loss: 3.215676784515381\n",
      "Epoch: 196; Model Loss: 3.2054715156555176\n",
      "Epoch: 197; Model Loss: 3.1953794956207275\n",
      "Epoch: 198; Model Loss: 3.1854376792907715\n",
      "Epoch: 199; Model Loss: 3.175619602203369\n",
      "Epoch: 200; Model Loss: 3.1659679412841797\n",
      "Epoch: 201; Model Loss: 3.156331777572632\n",
      "Epoch: 202; Model Loss: 3.146787405014038\n",
      "Epoch: 203; Model Loss: 3.1374242305755615\n",
      "Epoch: 204; Model Loss: 3.128178119659424\n",
      "Epoch: 205; Model Loss: 3.119020462036133\n",
      "Epoch: 206; Model Loss: 3.110002040863037\n",
      "Epoch: 207; Model Loss: 3.1011128425598145\n",
      "Epoch: 208; Model Loss: 3.092371702194214\n",
      "Epoch: 209; Model Loss: 3.0837466716766357\n",
      "Epoch: 210; Model Loss: 3.075197458267212\n",
      "Epoch: 211; Model Loss: 3.066732168197632\n",
      "Epoch: 212; Model Loss: 3.058377742767334\n",
      "Epoch: 213; Model Loss: 3.0501556396484375\n",
      "Epoch: 214; Model Loss: 3.042048454284668\n",
      "Epoch: 215; Model Loss: 3.034052848815918\n",
      "Epoch: 216; Model Loss: 3.0261714458465576\n",
      "Epoch: 217; Model Loss: 3.0183892250061035\n",
      "Epoch: 218; Model Loss: 3.0106942653656006\n",
      "Epoch: 219; Model Loss: 3.0031204223632812\n",
      "Epoch: 220; Model Loss: 2.995616912841797\n",
      "Epoch: 221; Model Loss: 2.9882335662841797\n",
      "Epoch: 222; Model Loss: 2.9809682369232178\n",
      "Epoch: 223; Model Loss: 2.973810911178589\n",
      "Epoch: 224; Model Loss: 2.966763973236084\n",
      "Epoch: 225; Model Loss: 2.9598095417022705\n",
      "Epoch: 226; Model Loss: 2.952942371368408\n",
      "Epoch: 227; Model Loss: 2.946159839630127\n",
      "Epoch: 228; Model Loss: 2.9394638538360596\n",
      "Epoch: 229; Model Loss: 2.932828426361084\n",
      "Epoch: 230; Model Loss: 2.9262611865997314\n",
      "Epoch: 231; Model Loss: 2.9197840690612793\n",
      "Epoch: 232; Model Loss: 2.9133718013763428\n",
      "Epoch: 233; Model Loss: 2.9070374965667725\n",
      "Epoch: 234; Model Loss: 2.9007813930511475\n",
      "Epoch: 235; Model Loss: 2.8945767879486084\n",
      "Epoch: 236; Model Loss: 2.8884520530700684\n",
      "Epoch: 237; Model Loss: 2.8823859691619873\n",
      "Epoch: 238; Model Loss: 2.8763628005981445\n",
      "Epoch: 239; Model Loss: 2.87042498588562\n",
      "Epoch: 240; Model Loss: 2.864560842514038\n",
      "Epoch: 241; Model Loss: 2.8587634563446045\n",
      "Epoch: 242; Model Loss: 2.8530352115631104\n",
      "Epoch: 243; Model Loss: 2.847353219985962\n",
      "Epoch: 244; Model Loss: 2.841686248779297\n",
      "Epoch: 245; Model Loss: 2.8360657691955566\n",
      "Epoch: 246; Model Loss: 2.8305304050445557\n",
      "Epoch: 247; Model Loss: 2.825063467025757\n",
      "Epoch: 248; Model Loss: 2.819662570953369\n",
      "Epoch: 249; Model Loss: 2.8143365383148193\n",
      "Epoch: 250; Model Loss: 2.809080123901367\n",
      "Epoch: 251; Model Loss: 2.803880214691162\n",
      "Epoch: 252; Model Loss: 2.7987310886383057\n",
      "Epoch: 253; Model Loss: 2.7936432361602783\n",
      "Epoch: 254; Model Loss: 2.788614511489868\n",
      "Epoch: 255; Model Loss: 2.7836480140686035\n",
      "Epoch: 256; Model Loss: 2.7787270545959473\n",
      "Epoch: 257; Model Loss: 2.773878812789917\n",
      "Epoch: 258; Model Loss: 2.7690982818603516\n",
      "Epoch: 259; Model Loss: 2.764371633529663\n",
      "Epoch: 260; Model Loss: 2.7596824169158936\n",
      "Epoch: 261; Model Loss: 2.7550415992736816\n",
      "Epoch: 262; Model Loss: 2.750399112701416\n",
      "Epoch: 263; Model Loss: 2.7458019256591797\n",
      "Epoch: 264; Model Loss: 2.741239309310913\n",
      "Epoch: 265; Model Loss: 2.736731767654419\n",
      "Epoch: 266; Model Loss: 2.7322802543640137\n",
      "Epoch: 267; Model Loss: 2.7278876304626465\n",
      "Epoch: 268; Model Loss: 2.7235448360443115\n",
      "Epoch: 269; Model Loss: 2.719254970550537\n",
      "Epoch: 270; Model Loss: 2.715014934539795\n",
      "Epoch: 271; Model Loss: 2.710817337036133\n",
      "Epoch: 272; Model Loss: 2.7066638469696045\n",
      "Epoch: 273; Model Loss: 2.7025232315063477\n",
      "Epoch: 274; Model Loss: 2.698427438735962\n",
      "Epoch: 275; Model Loss: 2.6943867206573486\n",
      "Epoch: 276; Model Loss: 2.690394163131714\n",
      "Epoch: 277; Model Loss: 2.6864469051361084\n",
      "Epoch: 278; Model Loss: 2.682553291320801\n",
      "Epoch: 279; Model Loss: 2.678687810897827\n",
      "Epoch: 280; Model Loss: 2.6748385429382324\n",
      "Epoch: 281; Model Loss: 2.671025276184082\n",
      "Epoch: 282; Model Loss: 2.667250156402588\n",
      "Epoch: 283; Model Loss: 2.6635184288024902\n",
      "Epoch: 284; Model Loss: 2.659822940826416\n",
      "Epoch: 285; Model Loss: 2.6561505794525146\n",
      "Epoch: 286; Model Loss: 2.6525182723999023\n",
      "Epoch: 287; Model Loss: 2.6489131450653076\n",
      "Epoch: 288; Model Loss: 2.6453423500061035\n",
      "Epoch: 289; Model Loss: 2.641810655593872\n",
      "Epoch: 290; Model Loss: 2.638324737548828\n",
      "Epoch: 291; Model Loss: 2.6348721981048584\n",
      "Epoch: 292; Model Loss: 2.631457567214966\n",
      "Epoch: 293; Model Loss: 2.6280786991119385\n",
      "Epoch: 294; Model Loss: 2.624742269515991\n",
      "Epoch: 295; Model Loss: 2.6214351654052734\n",
      "Epoch: 296; Model Loss: 2.6181299686431885\n",
      "Epoch: 297; Model Loss: 2.614856243133545\n",
      "Epoch: 298; Model Loss: 2.6116139888763428\n",
      "Epoch: 299; Model Loss: 2.6083998680114746\n",
      "Epoch: 300; Model Loss: 2.605203866958618\n",
      "Epoch: 301; Model Loss: 2.6019973754882812\n",
      "Epoch: 302; Model Loss: 2.598822593688965\n",
      "Epoch: 303; Model Loss: 2.595675468444824\n",
      "Epoch: 304; Model Loss: 2.5925557613372803\n",
      "Epoch: 305; Model Loss: 2.5894575119018555\n",
      "Epoch: 306; Model Loss: 2.586344003677368\n",
      "Epoch: 307; Model Loss: 2.5832650661468506\n",
      "Epoch: 308; Model Loss: 2.5802183151245117\n",
      "Epoch: 309; Model Loss: 2.577204704284668\n",
      "Epoch: 310; Model Loss: 2.57422137260437\n",
      "Epoch: 311; Model Loss: 2.571267604827881\n",
      "Epoch: 312; Model Loss: 2.5683205127716064\n",
      "Epoch: 313; Model Loss: 2.5654027462005615\n",
      "Epoch: 314; Model Loss: 2.5625061988830566\n",
      "Epoch: 315; Model Loss: 2.5596301555633545\n",
      "Epoch: 316; Model Loss: 2.5567853450775146\n",
      "Epoch: 317; Model Loss: 2.553973913192749\n",
      "Epoch: 318; Model Loss: 2.5511863231658936\n",
      "Epoch: 319; Model Loss: 2.5484230518341064\n",
      "Epoch: 320; Model Loss: 2.5456814765930176\n",
      "Epoch: 321; Model Loss: 2.542929172515869\n",
      "Epoch: 322; Model Loss: 2.540187120437622\n",
      "Epoch: 323; Model Loss: 2.5374715328216553\n",
      "Epoch: 324; Model Loss: 2.5347812175750732\n",
      "Epoch: 325; Model Loss: 2.5321061611175537\n",
      "Epoch: 326; Model Loss: 2.5294535160064697\n",
      "Epoch: 327; Model Loss: 2.5268239974975586\n",
      "Epoch: 328; Model Loss: 2.524224281311035\n",
      "Epoch: 329; Model Loss: 2.5216519832611084\n",
      "Epoch: 330; Model Loss: 2.519102096557617\n",
      "Epoch: 331; Model Loss: 2.5165789127349854\n",
      "Epoch: 332; Model Loss: 2.514082431793213\n",
      "Epoch: 333; Model Loss: 2.511608839035034\n",
      "Epoch: 334; Model Loss: 2.509155750274658\n",
      "Epoch: 335; Model Loss: 2.5067222118377686\n",
      "Epoch: 336; Model Loss: 2.5043082237243652\n",
      "Epoch: 337; Model Loss: 2.5019142627716064\n",
      "Epoch: 338; Model Loss: 2.499539375305176\n",
      "Epoch: 339; Model Loss: 2.497184991836548\n",
      "Epoch: 340; Model Loss: 2.49484920501709\n",
      "Epoch: 341; Model Loss: 2.492497682571411\n",
      "Epoch: 342; Model Loss: 2.490165948867798\n",
      "Epoch: 343; Model Loss: 2.487853527069092\n",
      "Epoch: 344; Model Loss: 2.4855620861053467\n",
      "Epoch: 345; Model Loss: 2.483293056488037\n",
      "Epoch: 346; Model Loss: 2.481046199798584\n",
      "Epoch: 347; Model Loss: 2.4788174629211426\n",
      "Epoch: 348; Model Loss: 2.476590156555176\n",
      "Epoch: 349; Model Loss: 2.4743785858154297\n",
      "Epoch: 350; Model Loss: 2.47216534614563\n",
      "Epoch: 351; Model Loss: 2.469968795776367\n",
      "Epoch: 352; Model Loss: 2.4677894115448\n",
      "Epoch: 353; Model Loss: 2.465622663497925\n",
      "Epoch: 354; Model Loss: 2.4634597301483154\n",
      "Epoch: 355; Model Loss: 2.461305856704712\n",
      "Epoch: 356; Model Loss: 2.4591760635375977\n",
      "Epoch: 357; Model Loss: 2.4570565223693848\n",
      "Epoch: 358; Model Loss: 2.454918146133423\n",
      "Epoch: 359; Model Loss: 2.4527835845947266\n",
      "Epoch: 360; Model Loss: 2.450650691986084\n",
      "Epoch: 361; Model Loss: 2.4485342502593994\n",
      "Epoch: 362; Model Loss: 2.4464328289031982\n",
      "Epoch: 363; Model Loss: 2.4443459510803223\n",
      "Epoch: 364; Model Loss: 2.4422593116760254\n",
      "Epoch: 365; Model Loss: 2.440159797668457\n",
      "Epoch: 366; Model Loss: 2.438074827194214\n",
      "Epoch: 367; Model Loss: 2.435997724533081\n",
      "Epoch: 368; Model Loss: 2.433917760848999\n",
      "Epoch: 369; Model Loss: 2.431857109069824\n",
      "Epoch: 370; Model Loss: 2.4298250675201416\n",
      "Epoch: 371; Model Loss: 2.4278101921081543\n",
      "Epoch: 372; Model Loss: 2.425807476043701\n",
      "Epoch: 373; Model Loss: 2.4238204956054688\n",
      "Epoch: 374; Model Loss: 2.4218456745147705\n",
      "Epoch: 375; Model Loss: 2.419889450073242\n",
      "Epoch: 376; Model Loss: 2.417945623397827\n",
      "Epoch: 377; Model Loss: 2.416013717651367\n",
      "Epoch: 378; Model Loss: 2.4140939712524414\n",
      "Epoch: 379; Model Loss: 2.4121851921081543\n",
      "Epoch: 380; Model Loss: 2.4102892875671387\n",
      "Epoch: 381; Model Loss: 2.4084103107452393\n",
      "Epoch: 382; Model Loss: 2.406550645828247\n",
      "Epoch: 383; Model Loss: 2.4047062397003174\n",
      "Epoch: 384; Model Loss: 2.4028730392456055\n",
      "Epoch: 385; Model Loss: 2.401052713394165\n",
      "Epoch: 386; Model Loss: 2.399243116378784\n",
      "Epoch: 387; Model Loss: 2.3974478244781494\n",
      "Epoch: 388; Model Loss: 2.3956713676452637\n",
      "Epoch: 389; Model Loss: 2.393908739089966\n",
      "Epoch: 390; Model Loss: 2.3921563625335693\n",
      "Epoch: 391; Model Loss: 2.3904178142547607\n",
      "Epoch: 392; Model Loss: 2.3886899948120117\n",
      "Epoch: 393; Model Loss: 2.3869643211364746\n",
      "Epoch: 394; Model Loss: 2.385244607925415\n",
      "Epoch: 395; Model Loss: 2.3835361003875732\n",
      "Epoch: 396; Model Loss: 2.3818399906158447\n",
      "Epoch: 397; Model Loss: 2.3801536560058594\n",
      "Epoch: 398; Model Loss: 2.37847638130188\n",
      "Epoch: 399; Model Loss: 2.3768088817596436\n",
      "Epoch: 400; Model Loss: 2.3751537799835205\n",
      "Epoch: 401; Model Loss: 2.3734984397888184\n",
      "Epoch: 402; Model Loss: 2.3718550205230713\n",
      "Epoch: 403; Model Loss: 2.3702194690704346\n",
      "Epoch: 404; Model Loss: 2.368587017059326\n",
      "Epoch: 405; Model Loss: 2.36696457862854\n",
      "Epoch: 406; Model Loss: 2.3653550148010254\n",
      "Epoch: 407; Model Loss: 2.3637616634368896\n",
      "Epoch: 408; Model Loss: 2.3621816635131836\n",
      "Epoch: 409; Model Loss: 2.360610008239746\n",
      "Epoch: 410; Model Loss: 2.3590376377105713\n",
      "Epoch: 411; Model Loss: 2.3574740886688232\n",
      "Epoch: 412; Model Loss: 2.355919599533081\n",
      "Epoch: 413; Model Loss: 2.3543756008148193\n",
      "Epoch: 414; Model Loss: 2.352842092514038\n",
      "Epoch: 415; Model Loss: 2.351316452026367\n",
      "Epoch: 416; Model Loss: 2.349799871444702\n",
      "Epoch: 417; Model Loss: 2.348294258117676\n",
      "Epoch: 418; Model Loss: 2.346796989440918\n",
      "Epoch: 419; Model Loss: 2.3453078269958496\n",
      "Epoch: 420; Model Loss: 2.3438286781311035\n",
      "Epoch: 421; Model Loss: 2.3423573970794678\n",
      "Epoch: 422; Model Loss: 2.3408946990966797\n",
      "Epoch: 423; Model Loss: 2.339442491531372\n",
      "Epoch: 424; Model Loss: 2.3379974365234375\n",
      "Epoch: 425; Model Loss: 2.336557388305664\n",
      "Epoch: 426; Model Loss: 2.3351173400878906\n",
      "Epoch: 427; Model Loss: 2.3336730003356934\n",
      "Epoch: 428; Model Loss: 2.3322362899780273\n",
      "Epoch: 429; Model Loss: 2.3308069705963135\n",
      "Epoch: 430; Model Loss: 2.329385995864868\n",
      "Epoch: 431; Model Loss: 2.327972650527954\n",
      "Epoch: 432; Model Loss: 2.326566457748413\n",
      "Epoch: 433; Model Loss: 2.3251678943634033\n",
      "Epoch: 434; Model Loss: 2.323777198791504\n",
      "Epoch: 435; Model Loss: 2.3223931789398193\n",
      "Epoch: 436; Model Loss: 2.321016788482666\n",
      "Epoch: 437; Model Loss: 2.319647789001465\n",
      "Epoch: 438; Model Loss: 2.3182873725891113\n",
      "Epoch: 439; Model Loss: 2.316934108734131\n",
      "Epoch: 440; Model Loss: 2.3155884742736816\n",
      "Epoch: 441; Model Loss: 2.3142499923706055\n",
      "Epoch: 442; Model Loss: 2.3129184246063232\n",
      "Epoch: 443; Model Loss: 2.311594009399414\n",
      "Epoch: 444; Model Loss: 2.3102762699127197\n",
      "Epoch: 445; Model Loss: 2.3089663982391357\n",
      "Epoch: 446; Model Loss: 2.3076634407043457\n",
      "Epoch: 447; Model Loss: 2.3063628673553467\n",
      "Epoch: 448; Model Loss: 2.3050708770751953\n",
      "Epoch: 449; Model Loss: 2.3037874698638916\n",
      "Epoch: 450; Model Loss: 2.302515745162964\n",
      "Epoch: 451; Model Loss: 2.301250696182251\n",
      "Epoch: 452; Model Loss: 2.2999908924102783\n",
      "Epoch: 453; Model Loss: 2.2987372875213623\n",
      "Epoch: 454; Model Loss: 2.2974894046783447\n",
      "Epoch: 455; Model Loss: 2.2962474822998047\n",
      "Epoch: 456; Model Loss: 2.2950117588043213\n",
      "Epoch: 457; Model Loss: 2.2937841415405273\n",
      "Epoch: 458; Model Loss: 2.292562246322632\n",
      "Epoch: 459; Model Loss: 2.2913482189178467\n",
      "Epoch: 460; Model Loss: 2.290140151977539\n",
      "Epoch: 461; Model Loss: 2.2889370918273926\n",
      "Epoch: 462; Model Loss: 2.2877378463745117\n",
      "Epoch: 463; Model Loss: 2.286539316177368\n",
      "Epoch: 464; Model Loss: 2.2853474617004395\n",
      "Epoch: 465; Model Loss: 2.2841625213623047\n",
      "Epoch: 466; Model Loss: 2.282986640930176\n",
      "Epoch: 467; Model Loss: 2.281818389892578\n",
      "Epoch: 468; Model Loss: 2.280656337738037\n",
      "Epoch: 469; Model Loss: 2.279500722885132\n",
      "Epoch: 470; Model Loss: 2.278351068496704\n",
      "Epoch: 471; Model Loss: 2.2772064208984375\n",
      "Epoch: 472; Model Loss: 2.276066541671753\n",
      "Epoch: 473; Model Loss: 2.274932384490967\n",
      "Epoch: 474; Model Loss: 2.273803234100342\n",
      "Epoch: 475; Model Loss: 2.272676706314087\n",
      "Epoch: 476; Model Loss: 2.271542549133301\n",
      "Epoch: 477; Model Loss: 2.270416259765625\n",
      "Epoch: 478; Model Loss: 2.2692975997924805\n",
      "Epoch: 479; Model Loss: 2.2681868076324463\n",
      "Epoch: 480; Model Loss: 2.267080545425415\n",
      "Epoch: 481; Model Loss: 2.2659802436828613\n",
      "Epoch: 482; Model Loss: 2.264887809753418\n",
      "Epoch: 483; Model Loss: 2.263800621032715\n",
      "Epoch: 484; Model Loss: 2.2627196311950684\n",
      "Epoch: 485; Model Loss: 2.2616446018218994\n",
      "Epoch: 486; Model Loss: 2.260576009750366\n",
      "Epoch: 487; Model Loss: 2.259512186050415\n",
      "Epoch: 488; Model Loss: 2.258453607559204\n",
      "Epoch: 489; Model Loss: 2.257401704788208\n",
      "Epoch: 490; Model Loss: 2.2563560009002686\n",
      "Epoch: 491; Model Loss: 2.2553153038024902\n",
      "Epoch: 492; Model Loss: 2.2542805671691895\n",
      "Epoch: 493; Model Loss: 2.2532503604888916\n",
      "Epoch: 494; Model Loss: 2.2522242069244385\n",
      "Epoch: 495; Model Loss: 2.251203775405884\n",
      "Epoch: 496; Model Loss: 2.2501885890960693\n",
      "Epoch: 497; Model Loss: 2.249178171157837\n",
      "Epoch: 498; Model Loss: 2.248173713684082\n",
      "Epoch: 499; Model Loss: 2.2471730709075928\n",
      "Epoch: 500; Model Loss: 2.2461769580841064\n",
      "Epoch: 501; Model Loss: 2.2451846599578857\n",
      "Epoch: 502; Model Loss: 2.244197130203247\n",
      "Epoch: 503; Model Loss: 2.243213653564453\n",
      "Epoch: 504; Model Loss: 2.242234468460083\n",
      "Epoch: 505; Model Loss: 2.2412588596343994\n",
      "Epoch: 506; Model Loss: 2.240285873413086\n",
      "Epoch: 507; Model Loss: 2.239316701889038\n",
      "Epoch: 508; Model Loss: 2.238347053527832\n",
      "Epoch: 509; Model Loss: 2.2373850345611572\n",
      "Epoch: 510; Model Loss: 2.2364275455474854\n",
      "Epoch: 511; Model Loss: 2.235474109649658\n",
      "Epoch: 512; Model Loss: 2.234524726867676\n",
      "Epoch: 513; Model Loss: 2.2335801124572754\n",
      "Epoch: 514; Model Loss: 2.2326412200927734\n",
      "Epoch: 515; Model Loss: 2.231705665588379\n",
      "Epoch: 516; Model Loss: 2.2307751178741455\n",
      "Epoch: 517; Model Loss: 2.2298498153686523\n",
      "Epoch: 518; Model Loss: 2.228928804397583\n",
      "Epoch: 519; Model Loss: 2.228010654449463\n",
      "Epoch: 520; Model Loss: 2.227093458175659\n",
      "Epoch: 521; Model Loss: 2.2261805534362793\n",
      "Epoch: 522; Model Loss: 2.2252750396728516\n",
      "Epoch: 523; Model Loss: 2.2243735790252686\n",
      "Epoch: 524; Model Loss: 2.223477840423584\n",
      "Epoch: 525; Model Loss: 2.222585678100586\n",
      "Epoch: 526; Model Loss: 2.221696615219116\n",
      "Epoch: 527; Model Loss: 2.220811367034912\n",
      "Epoch: 528; Model Loss: 2.2199296951293945\n",
      "Epoch: 529; Model Loss: 2.2190518379211426\n",
      "Epoch: 530; Model Loss: 2.2181766033172607\n",
      "Epoch: 531; Model Loss: 2.2173054218292236\n",
      "Epoch: 532; Model Loss: 2.216438055038452\n",
      "Epoch: 533; Model Loss: 2.21557354927063\n",
      "Epoch: 534; Model Loss: 2.2147130966186523\n",
      "Epoch: 535; Model Loss: 2.213855504989624\n",
      "Epoch: 536; Model Loss: 2.213001251220703\n",
      "Epoch: 537; Model Loss: 2.212141990661621\n",
      "Epoch: 538; Model Loss: 2.211286783218384\n",
      "Epoch: 539; Model Loss: 2.210434913635254\n",
      "Epoch: 540; Model Loss: 2.2095882892608643\n",
      "Epoch: 541; Model Loss: 2.2087461948394775\n",
      "Epoch: 542; Model Loss: 2.2079079151153564\n",
      "Epoch: 543; Model Loss: 2.207071542739868\n",
      "Epoch: 544; Model Loss: 2.206235647201538\n",
      "Epoch: 545; Model Loss: 2.205404043197632\n",
      "Epoch: 546; Model Loss: 2.204576253890991\n",
      "Epoch: 547; Model Loss: 2.2037513256073\n",
      "Epoch: 548; Model Loss: 2.202929973602295\n",
      "Epoch: 549; Model Loss: 2.2021121978759766\n",
      "Epoch: 550; Model Loss: 2.2012975215911865\n",
      "Epoch: 551; Model Loss: 2.200486421585083\n",
      "Epoch: 552; Model Loss: 2.199678421020508\n",
      "Epoch: 553; Model Loss: 2.198875904083252\n",
      "Epoch: 554; Model Loss: 2.198080539703369\n",
      "Epoch: 555; Model Loss: 2.197288990020752\n",
      "Epoch: 556; Model Loss: 2.196500062942505\n",
      "Epoch: 557; Model Loss: 2.1957147121429443\n",
      "Epoch: 558; Model Loss: 2.194931745529175\n",
      "Epoch: 559; Model Loss: 2.1941492557525635\n",
      "Epoch: 560; Model Loss: 2.1933705806732178\n",
      "Epoch: 561; Model Loss: 2.192596435546875\n",
      "Epoch: 562; Model Loss: 2.1918258666992188\n",
      "Epoch: 563; Model Loss: 2.1910581588745117\n",
      "Epoch: 564; Model Loss: 2.190293550491333\n",
      "Epoch: 565; Model Loss: 2.189532518386841\n",
      "Epoch: 566; Model Loss: 2.188774347305298\n",
      "Epoch: 567; Model Loss: 2.1880221366882324\n",
      "Epoch: 568; Model Loss: 2.1872754096984863\n",
      "Epoch: 569; Model Loss: 2.186532497406006\n",
      "Epoch: 570; Model Loss: 2.185793161392212\n",
      "Epoch: 571; Model Loss: 2.185056686401367\n",
      "Epoch: 572; Model Loss: 2.1843228340148926\n",
      "Epoch: 573; Model Loss: 2.1835923194885254\n",
      "Epoch: 574; Model Loss: 2.1828651428222656\n",
      "Epoch: 575; Model Loss: 2.182140588760376\n",
      "Epoch: 576; Model Loss: 2.181419610977173\n",
      "Epoch: 577; Model Loss: 2.1807010173797607\n",
      "Epoch: 578; Model Loss: 2.179981231689453\n",
      "Epoch: 579; Model Loss: 2.179264545440674\n",
      "Epoch: 580; Model Loss: 2.1785507202148438\n",
      "Epoch: 581; Model Loss: 2.177840232849121\n",
      "Epoch: 582; Model Loss: 2.1771326065063477\n",
      "Epoch: 583; Model Loss: 2.1764276027679443\n",
      "Epoch: 584; Model Loss: 2.1757261753082275\n",
      "Epoch: 585; Model Loss: 2.1750271320343018\n",
      "Epoch: 586; Model Loss: 2.1743314266204834\n",
      "Epoch: 587; Model Loss: 2.1736388206481934\n",
      "Epoch: 588; Model Loss: 2.1729485988616943\n",
      "Epoch: 589; Model Loss: 2.17226243019104\n",
      "Epoch: 590; Model Loss: 2.171579360961914\n",
      "Epoch: 591; Model Loss: 2.17089581489563\n",
      "Epoch: 592; Model Loss: 2.1702094078063965\n",
      "Epoch: 593; Model Loss: 2.169517755508423\n",
      "Epoch: 594; Model Loss: 2.1688296794891357\n",
      "Epoch: 595; Model Loss: 2.1681439876556396\n",
      "Epoch: 596; Model Loss: 2.167461633682251\n",
      "Epoch: 597; Model Loss: 2.1667823791503906\n",
      "Epoch: 598; Model Loss: 2.1661057472229004\n",
      "Epoch: 599; Model Loss: 2.1654269695281982\n",
      "Epoch: 600; Model Loss: 2.1647510528564453\n",
      "Epoch: 601; Model Loss: 2.164079427719116\n",
      "Epoch: 602; Model Loss: 2.1634106636047363\n",
      "Epoch: 603; Model Loss: 2.162745475769043\n",
      "Epoch: 604; Model Loss: 2.162086248397827\n",
      "Epoch: 605; Model Loss: 2.1614294052124023\n",
      "Epoch: 606; Model Loss: 2.1607754230499268\n",
      "Epoch: 607; Model Loss: 2.1601245403289795\n",
      "Epoch: 608; Model Loss: 2.1594767570495605\n",
      "Epoch: 609; Model Loss: 2.1588337421417236\n",
      "Epoch: 610; Model Loss: 2.1581931114196777\n",
      "Epoch: 611; Model Loss: 2.157555341720581\n",
      "Epoch: 612; Model Loss: 2.156921148300171\n",
      "Epoch: 613; Model Loss: 2.156280755996704\n",
      "Epoch: 614; Model Loss: 2.155642509460449\n",
      "Epoch: 615; Model Loss: 2.15500807762146\n",
      "Epoch: 616; Model Loss: 2.1543760299682617\n",
      "Epoch: 617; Model Loss: 2.1537466049194336\n",
      "Epoch: 618; Model Loss: 2.153120279312134\n",
      "Epoch: 619; Model Loss: 2.152496814727783\n",
      "Epoch: 620; Model Loss: 2.1518759727478027\n",
      "Epoch: 621; Model Loss: 2.1512579917907715\n",
      "Epoch: 622; Model Loss: 2.150642156600952\n",
      "Epoch: 623; Model Loss: 2.1500210762023926\n",
      "Epoch: 624; Model Loss: 2.1494033336639404\n",
      "Epoch: 625; Model Loss: 2.1487877368927\n",
      "Epoch: 626; Model Loss: 2.148175001144409\n",
      "Epoch: 627; Model Loss: 2.147564649581909\n",
      "Epoch: 628; Model Loss: 2.1469573974609375\n",
      "Epoch: 629; Model Loss: 2.1463541984558105\n",
      "Epoch: 630; Model Loss: 2.145754814147949\n",
      "Epoch: 631; Model Loss: 2.1451616287231445\n",
      "Epoch: 632; Model Loss: 2.1445722579956055\n",
      "Epoch: 633; Model Loss: 2.1439857482910156\n",
      "Epoch: 634; Model Loss: 2.1434009075164795\n",
      "Epoch: 635; Model Loss: 2.1428182125091553\n",
      "Epoch: 636; Model Loss: 2.142237663269043\n",
      "Epoch: 637; Model Loss: 2.141659736633301\n",
      "Epoch: 638; Model Loss: 2.1410839557647705\n",
      "Epoch: 639; Model Loss: 2.1405107975006104\n",
      "Epoch: 640; Model Loss: 2.139939785003662\n",
      "Epoch: 641; Model Loss: 2.1393706798553467\n",
      "Epoch: 642; Model Loss: 2.1388039588928223\n",
      "Epoch: 643; Model Loss: 2.1382389068603516\n",
      "Epoch: 644; Model Loss: 2.137676477432251\n",
      "Epoch: 645; Model Loss: 2.1371161937713623\n",
      "Epoch: 646; Model Loss: 2.1365578174591064\n",
      "Epoch: 647; Model Loss: 2.1360018253326416\n",
      "Epoch: 648; Model Loss: 2.1354479789733887\n",
      "Epoch: 649; Model Loss: 2.1348960399627686\n",
      "Epoch: 650; Model Loss: 2.1343460083007812\n",
      "Epoch: 651; Model Loss: 2.133798599243164\n",
      "Epoch: 652; Model Loss: 2.133253812789917\n",
      "Epoch: 653; Model Loss: 2.1327145099639893\n",
      "Epoch: 654; Model Loss: 2.1321771144866943\n",
      "Epoch: 655; Model Loss: 2.1316418647766113\n",
      "Epoch: 656; Model Loss: 2.131108283996582\n",
      "Epoch: 657; Model Loss: 2.1305770874023438\n",
      "Epoch: 658; Model Loss: 2.130047559738159\n",
      "Epoch: 659; Model Loss: 2.1295199394226074\n",
      "Epoch: 660; Model Loss: 2.128995418548584\n",
      "Epoch: 661; Model Loss: 2.1284735202789307\n",
      "Epoch: 662; Model Loss: 2.1279542446136475\n",
      "Epoch: 663; Model Loss: 2.1274378299713135\n",
      "Epoch: 664; Model Loss: 2.126923084259033\n",
      "Epoch: 665; Model Loss: 2.126410484313965\n",
      "Epoch: 666; Model Loss: 2.1259005069732666\n",
      "Epoch: 667; Model Loss: 2.125392436981201\n",
      "Epoch: 668; Model Loss: 2.1248865127563477\n",
      "Epoch: 669; Model Loss: 2.1243817806243896\n",
      "Epoch: 670; Model Loss: 2.1238794326782227\n",
      "Epoch: 671; Model Loss: 2.1233787536621094\n",
      "Epoch: 672; Model Loss: 2.1228792667388916\n",
      "Epoch: 673; Model Loss: 2.122382402420044\n",
      "Epoch: 674; Model Loss: 2.121886968612671\n",
      "Epoch: 675; Model Loss: 2.1213934421539307\n",
      "Epoch: 676; Model Loss: 2.120901584625244\n",
      "Epoch: 677; Model Loss: 2.1204113960266113\n",
      "Epoch: 678; Model Loss: 2.1199228763580322\n",
      "Epoch: 679; Model Loss: 2.119436264038086\n",
      "Epoch: 680; Model Loss: 2.1189513206481934\n",
      "Epoch: 681; Model Loss: 2.1184682846069336\n",
      "Epoch: 682; Model Loss: 2.1179869174957275\n",
      "Epoch: 683; Model Loss: 2.117506504058838\n",
      "Epoch: 684; Model Loss: 2.117027759552002\n",
      "Epoch: 685; Model Loss: 2.116550922393799\n",
      "Epoch: 686; Model Loss: 2.116075277328491\n",
      "Epoch: 687; Model Loss: 2.1156017780303955\n",
      "Epoch: 688; Model Loss: 2.1151297092437744\n",
      "Epoch: 689; Model Loss: 2.114659309387207\n",
      "Epoch: 690; Model Loss: 2.1141908168792725\n",
      "Epoch: 691; Model Loss: 2.113725423812866\n",
      "Epoch: 692; Model Loss: 2.1132616996765137\n",
      "Epoch: 693; Model Loss: 2.112799644470215\n",
      "Epoch: 694; Model Loss: 2.1123390197753906\n",
      "Epoch: 695; Model Loss: 2.111879825592041\n",
      "Epoch: 696; Model Loss: 2.1114230155944824\n",
      "Epoch: 697; Model Loss: 2.110968589782715\n",
      "Epoch: 698; Model Loss: 2.11051607131958\n",
      "Epoch: 699; Model Loss: 2.11006498336792\n",
      "Epoch: 700; Model Loss: 2.1096150875091553\n",
      "Epoch: 701; Model Loss: 2.1091666221618652\n",
      "Epoch: 702; Model Loss: 2.108719825744629\n",
      "Epoch: 703; Model Loss: 2.1082746982574463\n",
      "Epoch: 704; Model Loss: 2.107830762863159\n",
      "Epoch: 705; Model Loss: 2.1073882579803467\n",
      "Epoch: 706; Model Loss: 2.106947660446167\n",
      "Epoch: 707; Model Loss: 2.1065077781677246\n",
      "Epoch: 708; Model Loss: 2.106069803237915\n",
      "Epoch: 709; Model Loss: 2.105633020401001\n",
      "Epoch: 710; Model Loss: 2.1051976680755615\n",
      "Epoch: 711; Model Loss: 2.1047632694244385\n",
      "Epoch: 712; Model Loss: 2.1043312549591064\n",
      "Epoch: 713; Model Loss: 2.1039016246795654\n",
      "Epoch: 714; Model Loss: 2.1034746170043945\n",
      "Epoch: 715; Model Loss: 2.1030492782592773\n",
      "Epoch: 716; Model Loss: 2.1026253700256348\n",
      "Epoch: 717; Model Loss: 2.102202892303467\n",
      "Epoch: 718; Model Loss: 2.1017813682556152\n",
      "Epoch: 719; Model Loss: 2.1013617515563965\n",
      "Epoch: 720; Model Loss: 2.100942850112915\n",
      "Epoch: 721; Model Loss: 2.100525379180908\n",
      "Epoch: 722; Model Loss: 2.100109100341797\n",
      "Epoch: 723; Model Loss: 2.0996947288513184\n",
      "Epoch: 724; Model Loss: 2.0992825031280518\n",
      "Epoch: 725; Model Loss: 2.0988714694976807\n",
      "Epoch: 726; Model Loss: 2.0984625816345215\n",
      "Epoch: 727; Model Loss: 2.0980546474456787\n",
      "Epoch: 728; Model Loss: 2.0976481437683105\n",
      "Epoch: 729; Model Loss: 2.097242832183838\n",
      "Epoch: 730; Model Loss: 2.0968387126922607\n",
      "Epoch: 731; Model Loss: 2.096435546875\n",
      "Epoch: 732; Model Loss: 2.096033811569214\n",
      "Epoch: 733; Model Loss: 2.0956342220306396\n",
      "Epoch: 734; Model Loss: 2.0952367782592773\n",
      "Epoch: 735; Model Loss: 2.0948407649993896\n",
      "Epoch: 736; Model Loss: 2.0944457054138184\n",
      "Epoch: 737; Model Loss: 2.0940518379211426\n",
      "Epoch: 738; Model Loss: 2.0936594009399414\n",
      "Epoch: 739; Model Loss: 2.0932674407958984\n",
      "Epoch: 740; Model Loss: 2.09287691116333\n",
      "Epoch: 741; Model Loss: 2.0924882888793945\n",
      "Epoch: 742; Model Loss: 2.0921010971069336\n",
      "Epoch: 743; Model Loss: 2.09171462059021\n",
      "Epoch: 744; Model Loss: 2.0913290977478027\n",
      "Epoch: 745; Model Loss: 2.09094500541687\n",
      "Epoch: 746; Model Loss: 2.090561628341675\n",
      "Epoch: 747; Model Loss: 2.090179204940796\n",
      "Epoch: 748; Model Loss: 2.0897939205169678\n",
      "Epoch: 749; Model Loss: 2.0894100666046143\n",
      "Epoch: 750; Model Loss: 2.089027166366577\n",
      "Epoch: 751; Model Loss: 2.0886449813842773\n",
      "Epoch: 752; Model Loss: 2.088259220123291\n",
      "Epoch: 753; Model Loss: 2.0878748893737793\n",
      "Epoch: 754; Model Loss: 2.087491512298584\n",
      "Epoch: 755; Model Loss: 2.087109088897705\n",
      "Epoch: 756; Model Loss: 2.0867300033569336\n",
      "Epoch: 757; Model Loss: 2.0863513946533203\n",
      "Epoch: 758; Model Loss: 2.0859739780426025\n",
      "Epoch: 759; Model Loss: 2.0855977535247803\n",
      "Epoch: 760; Model Loss: 2.0852224826812744\n",
      "Epoch: 761; Model Loss: 2.084848165512085\n",
      "Epoch: 762; Model Loss: 2.084474563598633\n",
      "Epoch: 763; Model Loss: 2.084104299545288\n",
      "Epoch: 764; Model Loss: 2.083737373352051\n",
      "Epoch: 765; Model Loss: 2.083371162414551\n",
      "Epoch: 766; Model Loss: 2.083005905151367\n",
      "Epoch: 767; Model Loss: 2.082641839981079\n",
      "Epoch: 768; Model Loss: 2.08227801322937\n",
      "Epoch: 769; Model Loss: 2.0819125175476074\n",
      "Epoch: 770; Model Loss: 2.081547498703003\n",
      "Epoch: 771; Model Loss: 2.081183910369873\n",
      "Epoch: 772; Model Loss: 2.0808210372924805\n",
      "Epoch: 773; Model Loss: 2.0804591178894043\n",
      "Epoch: 774; Model Loss: 2.0800976753234863\n",
      "Epoch: 775; Model Loss: 2.0797359943389893\n",
      "Epoch: 776; Model Loss: 2.0793752670288086\n",
      "Epoch: 777; Model Loss: 2.0790157318115234\n",
      "Epoch: 778; Model Loss: 2.0786566734313965\n",
      "Epoch: 779; Model Loss: 2.078299045562744\n",
      "Epoch: 780; Model Loss: 2.077942132949829\n",
      "Epoch: 781; Model Loss: 2.0775859355926514\n",
      "Epoch: 782; Model Loss: 2.07723069190979\n",
      "Epoch: 783; Model Loss: 2.076876640319824\n",
      "Epoch: 784; Model Loss: 2.0765228271484375\n",
      "Epoch: 785; Model Loss: 2.0761704444885254\n",
      "Epoch: 786; Model Loss: 2.0758187770843506\n",
      "Epoch: 787; Model Loss: 2.0754687786102295\n",
      "Epoch: 788; Model Loss: 2.0751194953918457\n",
      "Epoch: 789; Model Loss: 2.0747714042663574\n",
      "Epoch: 790; Model Loss: 2.0744237899780273\n",
      "Epoch: 791; Model Loss: 2.074077606201172\n",
      "Epoch: 792; Model Loss: 2.0737318992614746\n",
      "Epoch: 793; Model Loss: 2.0733871459960938\n",
      "Epoch: 794; Model Loss: 2.0730433464050293\n",
      "Epoch: 795; Model Loss: 2.0727005004882812\n",
      "Epoch: 796; Model Loss: 2.0723583698272705\n",
      "Epoch: 797; Model Loss: 2.072017192840576\n",
      "Epoch: 798; Model Loss: 2.071678638458252\n",
      "Epoch: 799; Model Loss: 2.071340560913086\n",
      "Epoch: 800; Model Loss: 2.0710036754608154\n",
      "Epoch: 801; Model Loss: 2.070667266845703\n",
      "Epoch: 802; Model Loss: 2.0703320503234863\n",
      "Epoch: 803; Model Loss: 2.0699973106384277\n",
      "Epoch: 804; Model Loss: 2.0696637630462646\n",
      "Epoch: 805; Model Loss: 2.069330930709839\n",
      "Epoch: 806; Model Loss: 2.0689990520477295\n",
      "Epoch: 807; Model Loss: 2.0686683654785156\n",
      "Epoch: 808; Model Loss: 2.0683391094207764\n",
      "Epoch: 809; Model Loss: 2.0680103302001953\n",
      "Epoch: 810; Model Loss: 2.0676825046539307\n",
      "Epoch: 811; Model Loss: 2.067355155944824\n",
      "Epoch: 812; Model Loss: 2.0670294761657715\n",
      "Epoch: 813; Model Loss: 2.0667049884796143\n",
      "Epoch: 814; Model Loss: 2.0663819313049316\n",
      "Epoch: 815; Model Loss: 2.0660595893859863\n",
      "Epoch: 816; Model Loss: 2.065739393234253\n",
      "Epoch: 817; Model Loss: 2.0654196739196777\n",
      "Epoch: 818; Model Loss: 2.065100908279419\n",
      "Epoch: 819; Model Loss: 2.0647826194763184\n",
      "Epoch: 820; Model Loss: 2.0644655227661133\n",
      "Epoch: 821; Model Loss: 2.0641491413116455\n",
      "Epoch: 822; Model Loss: 2.063833475112915\n",
      "Epoch: 823; Model Loss: 2.063518762588501\n",
      "Epoch: 824; Model Loss: 2.0632050037384033\n",
      "Epoch: 825; Model Loss: 2.062891960144043\n",
      "Epoch: 826; Model Loss: 2.06257963180542\n",
      "Epoch: 827; Model Loss: 2.062267303466797\n",
      "Epoch: 828; Model Loss: 2.0619499683380127\n",
      "Epoch: 829; Model Loss: 2.061633586883545\n",
      "Epoch: 830; Model Loss: 2.0613181591033936\n",
      "Epoch: 831; Model Loss: 2.0610036849975586\n",
      "Epoch: 832; Model Loss: 2.060689687728882\n",
      "Epoch: 833; Model Loss: 2.0603766441345215\n",
      "Epoch: 834; Model Loss: 2.0600647926330566\n",
      "Epoch: 835; Model Loss: 2.0597541332244873\n",
      "Epoch: 836; Model Loss: 2.0594441890716553\n",
      "Epoch: 837; Model Loss: 2.0591349601745605\n",
      "Epoch: 838; Model Loss: 2.0588266849517822\n",
      "Epoch: 839; Model Loss: 2.0585200786590576\n",
      "Epoch: 840; Model Loss: 2.0582141876220703\n",
      "Epoch: 841; Model Loss: 2.0579090118408203\n",
      "Epoch: 842; Model Loss: 2.057605028152466\n",
      "Epoch: 843; Model Loss: 2.0573017597198486\n",
      "Epoch: 844; Model Loss: 2.0570003986358643\n",
      "Epoch: 845; Model Loss: 2.056699514389038\n",
      "Epoch: 846; Model Loss: 2.0564000606536865\n",
      "Epoch: 847; Model Loss: 2.056100845336914\n",
      "Epoch: 848; Model Loss: 2.055802822113037\n",
      "Epoch: 849; Model Loss: 2.0555052757263184\n",
      "Epoch: 850; Model Loss: 2.055208683013916\n",
      "Epoch: 851; Model Loss: 2.054913282394409\n",
      "Epoch: 852; Model Loss: 2.0546183586120605\n",
      "Epoch: 853; Model Loss: 2.0543243885040283\n",
      "Epoch: 854; Model Loss: 2.0540311336517334\n",
      "Epoch: 855; Model Loss: 2.053738832473755\n",
      "Epoch: 856; Model Loss: 2.053448438644409\n",
      "Epoch: 857; Model Loss: 2.053158760070801\n",
      "Epoch: 858; Model Loss: 2.052870750427246\n",
      "Epoch: 859; Model Loss: 2.0525829792022705\n",
      "Epoch: 860; Model Loss: 2.0522961616516113\n",
      "Epoch: 861; Model Loss: 2.0520102977752686\n",
      "Epoch: 862; Model Loss: 2.0517256259918213\n",
      "Epoch: 863; Model Loss: 2.051441192626953\n",
      "Epoch: 864; Model Loss: 2.0511574745178223\n",
      "Epoch: 865; Model Loss: 2.050874948501587\n",
      "Epoch: 866; Model Loss: 2.0505943298339844\n",
      "Epoch: 867; Model Loss: 2.05031418800354\n",
      "Epoch: 868; Model Loss: 2.050034999847412\n",
      "Epoch: 869; Model Loss: 2.0497562885284424\n",
      "Epoch: 870; Model Loss: 2.049478769302368\n",
      "Epoch: 871; Model Loss: 2.0492022037506104\n",
      "Epoch: 872; Model Loss: 2.0489261150360107\n",
      "Epoch: 873; Model Loss: 2.0486509799957275\n",
      "Epoch: 874; Model Loss: 2.0483763217926025\n",
      "Epoch: 875; Model Loss: 2.048102617263794\n",
      "Epoch: 876; Model Loss: 2.0478298664093018\n",
      "Epoch: 877; Model Loss: 2.047558069229126\n",
      "Epoch: 878; Model Loss: 2.0472867488861084\n",
      "Epoch: 879; Model Loss: 2.0470163822174072\n",
      "Epoch: 880; Model Loss: 2.0467467308044434\n",
      "Epoch: 881; Model Loss: 2.046477794647217\n",
      "Epoch: 882; Model Loss: 2.0462095737457275\n",
      "Epoch: 883; Model Loss: 2.045942544937134\n",
      "Epoch: 884; Model Loss: 2.0456771850585938\n",
      "Epoch: 885; Model Loss: 2.0454142093658447\n",
      "Epoch: 886; Model Loss: 2.045151472091675\n",
      "Epoch: 887; Model Loss: 2.0448896884918213\n",
      "Epoch: 888; Model Loss: 2.0446290969848633\n",
      "Epoch: 889; Model Loss: 2.044369697570801\n",
      "Epoch: 890; Model Loss: 2.0441110134124756\n",
      "Epoch: 891; Model Loss: 2.043853282928467\n",
      "Epoch: 892; Model Loss: 2.0435965061187744\n",
      "Epoch: 893; Model Loss: 2.0433409214019775\n",
      "Epoch: 894; Model Loss: 2.0430853366851807\n",
      "Epoch: 895; Model Loss: 2.0428314208984375\n",
      "Epoch: 896; Model Loss: 2.0425777435302734\n",
      "Epoch: 897; Model Loss: 2.0423247814178467\n",
      "Epoch: 898; Model Loss: 2.0420725345611572\n",
      "Epoch: 899; Model Loss: 2.0418214797973633\n",
      "Epoch: 900; Model Loss: 2.0415709018707275\n",
      "Epoch: 901; Model Loss: 2.041321039199829\n",
      "Epoch: 902; Model Loss: 2.041071653366089\n",
      "Epoch: 903; Model Loss: 2.040823221206665\n",
      "Epoch: 904; Model Loss: 2.0405757427215576\n",
      "Epoch: 905; Model Loss: 2.0403289794921875\n",
      "Epoch: 906; Model Loss: 2.0400829315185547\n",
      "Epoch: 907; Model Loss: 2.0398380756378174\n",
      "Epoch: 908; Model Loss: 2.0395941734313965\n",
      "Epoch: 909; Model Loss: 2.039351224899292\n",
      "Epoch: 910; Model Loss: 2.0391087532043457\n",
      "Epoch: 911; Model Loss: 2.0388667583465576\n",
      "Epoch: 912; Model Loss: 2.038625717163086\n",
      "Epoch: 913; Model Loss: 2.0383858680725098\n",
      "Epoch: 914; Model Loss: 2.038146495819092\n",
      "Epoch: 915; Model Loss: 2.037907838821411\n",
      "Epoch: 916; Model Loss: 2.0376698970794678\n",
      "Epoch: 917; Model Loss: 2.037432909011841\n",
      "Epoch: 918; Model Loss: 2.037196397781372\n",
      "Epoch: 919; Model Loss: 2.0369606018066406\n",
      "Epoch: 920; Model Loss: 2.0367257595062256\n",
      "Epoch: 921; Model Loss: 2.0364913940429688\n",
      "Epoch: 922; Model Loss: 2.03625750541687\n",
      "Epoch: 923; Model Loss: 2.036025285720825\n",
      "Epoch: 924; Model Loss: 2.035792827606201\n",
      "Epoch: 925; Model Loss: 2.0355613231658936\n",
      "Epoch: 926; Model Loss: 2.0353310108184814\n",
      "Epoch: 927; Model Loss: 2.0351006984710693\n",
      "Epoch: 928; Model Loss: 2.0348715782165527\n",
      "Epoch: 929; Model Loss: 2.0346431732177734\n",
      "Epoch: 930; Model Loss: 2.0344152450561523\n",
      "Epoch: 931; Model Loss: 2.0341877937316895\n",
      "Epoch: 932; Model Loss: 2.033961534500122\n",
      "Epoch: 933; Model Loss: 2.033735752105713\n",
      "Epoch: 934; Model Loss: 2.033510446548462\n",
      "Epoch: 935; Model Loss: 2.0332860946655273\n",
      "Epoch: 936; Model Loss: 2.033062696456909\n",
      "Epoch: 937; Model Loss: 2.03283953666687\n",
      "Epoch: 938; Model Loss: 2.0326175689697266\n",
      "Epoch: 939; Model Loss: 2.0323965549468994\n",
      "Epoch: 940; Model Loss: 2.0321767330169678\n",
      "Epoch: 941; Model Loss: 2.0319583415985107\n",
      "Epoch: 942; Model Loss: 2.031740188598633\n",
      "Epoch: 943; Model Loss: 2.0315232276916504\n",
      "Epoch: 944; Model Loss: 2.031306266784668\n",
      "Epoch: 945; Model Loss: 2.031090497970581\n",
      "Epoch: 946; Model Loss: 2.0308752059936523\n",
      "Epoch: 947; Model Loss: 2.030660629272461\n",
      "Epoch: 948; Model Loss: 2.030446767807007\n",
      "Epoch: 949; Model Loss: 2.030233144760132\n",
      "Epoch: 950; Model Loss: 2.0300204753875732\n",
      "Epoch: 951; Model Loss: 2.029808759689331\n",
      "Epoch: 952; Model Loss: 2.029597282409668\n",
      "Epoch: 953; Model Loss: 2.0293867588043213\n",
      "Epoch: 954; Model Loss: 2.029176712036133\n",
      "Epoch: 955; Model Loss: 2.0289673805236816\n",
      "Epoch: 956; Model Loss: 2.028759479522705\n",
      "Epoch: 957; Model Loss: 2.028552293777466\n",
      "Epoch: 958; Model Loss: 2.028346538543701\n",
      "Epoch: 959; Model Loss: 2.0281410217285156\n",
      "Epoch: 960; Model Loss: 2.0279359817504883\n",
      "Epoch: 961; Model Loss: 2.0277318954467773\n",
      "Epoch: 962; Model Loss: 2.0275280475616455\n",
      "Epoch: 963; Model Loss: 2.02732515335083\n",
      "Epoch: 964; Model Loss: 2.027122735977173\n",
      "Epoch: 965; Model Loss: 2.026921033859253\n",
      "Epoch: 966; Model Loss: 2.026719808578491\n",
      "Epoch: 967; Model Loss: 2.026519298553467\n",
      "Epoch: 968; Model Loss: 2.0263195037841797\n",
      "Epoch: 969; Model Loss: 2.0261199474334717\n",
      "Epoch: 970; Model Loss: 2.02592134475708\n",
      "Epoch: 971; Model Loss: 2.025723457336426\n",
      "Epoch: 972; Model Loss: 2.025526285171509\n",
      "Epoch: 973; Model Loss: 2.025330066680908\n",
      "Epoch: 974; Model Loss: 2.025135040283203\n",
      "Epoch: 975; Model Loss: 2.0249407291412354\n",
      "Epoch: 976; Model Loss: 2.024747371673584\n",
      "Epoch: 977; Model Loss: 2.0245540142059326\n",
      "Epoch: 978; Model Loss: 2.0243616104125977\n",
      "Epoch: 979; Model Loss: 2.024169445037842\n",
      "Epoch: 980; Model Loss: 2.0239782333374023\n",
      "Epoch: 981; Model Loss: 2.023787498474121\n",
      "Epoch: 982; Model Loss: 2.023597240447998\n",
      "Epoch: 983; Model Loss: 2.023407459259033\n",
      "Epoch: 984; Model Loss: 2.0232186317443848\n",
      "Epoch: 985; Model Loss: 2.0230302810668945\n",
      "Epoch: 986; Model Loss: 2.0228424072265625\n",
      "Epoch: 987; Model Loss: 2.0226550102233887\n",
      "Epoch: 988; Model Loss: 2.022468328475952\n",
      "Epoch: 989; Model Loss: 2.022282361984253\n",
      "Epoch: 990; Model Loss: 2.022097110748291\n",
      "Epoch: 991; Model Loss: 2.0219132900238037\n",
      "Epoch: 992; Model Loss: 2.0217301845550537\n",
      "Epoch: 993; Model Loss: 2.021547555923462\n",
      "Epoch: 994; Model Loss: 2.0213656425476074\n",
      "Epoch: 995; Model Loss: 2.021183729171753\n",
      "Epoch: 996; Model Loss: 2.021003007888794\n",
      "Epoch: 997; Model Loss: 2.0208232402801514\n",
      "Epoch: 998; Model Loss: 2.020643472671509\n",
      "Epoch: 999; Model Loss: 2.0204646587371826\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "lr = 0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # DO FORWARD PASS\n",
    "    # Calculate first hidden layer outputs\n",
    "    H_1 = ((A_HAT @ X) @ W_0).relu()  # H_1 reduces feature vector space from 1433 -> 100\n",
    "\n",
    "    # Calculate Output from our 2-layer GCN\n",
    "    O = ((A_HAT @ H_1) @ W_1).relu()\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = CrossEntropyLoss()\n",
    "    model_loss = loss(O, Y_ONE_HOT)\n",
    "\n",
    "    print(f\"Epoch: {epoch}; Model Loss: {model_loss.item()}\")\n",
    "\n",
    "    W_0.grad = None\n",
    "    W_1.grad = None\n",
    "\n",
    "    # DO BACKWARD PASS\n",
    "    model_loss.backward()\n",
    "    W_0.data -= lr * W_0.grad\n",
    "    W_1.data -= lr * W_1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_neurons = 100\n",
    "vocab_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x113173090>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_a = torch.randn(\n",
    "    n_hidden_neurons,\n",
    "    vocab_size,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6007, -0.0498, -0.0094,  ..., -1.3090,  0.1370, -0.4259],\n",
       "        [-1.6263,  0.3625, -0.0152,  ...,  0.0912,  0.0984,  0.6065],\n",
       "        [ 0.5099,  1.9434,  1.4643,  ..., -0.5361,  0.7526, -0.9205],\n",
       "        ...,\n",
       "        [ 0.4078,  1.2127, -1.1502,  ..., -0.4776, -0.6093, -1.1972],\n",
       "        [ 1.2646,  1.0890, -0.8633,  ..., -0.0072, -0.8836, -0.0283],\n",
       "        [-0.2326,  0.5926, -0.1806,  ...,  0.0436, -1.6337, -0.1688]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_a = np.random.randn(n_hidden_neurons, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.76405235,  0.40015721,  0.97873798, ...,  0.8644362 ,\n",
       "        -0.74216502,  2.26975462],\n",
       "       [-1.45436567,  0.04575852, -0.18718385, ...,  0.77749036,\n",
       "        -1.61389785, -0.21274028],\n",
       "       [-0.89546656,  0.3869025 , -0.51080514, ...,  1.13940068,\n",
       "        -1.23482582,  0.40234164],\n",
       "       ...,\n",
       "       [-0.54735557, -0.55079943,  0.7920415 , ..., -0.47360406,\n",
       "         0.30365647,  1.03395699],\n",
       "       [ 1.90934263,  1.66387312,  0.90082276, ..., -2.31377311,\n",
       "        -0.8425717 , -1.54292145],\n",
       "       [-0.40176374, -0.4152314 , -0.67366417, ...,  1.13638808,\n",
       "         0.67161657, -0.97416744]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('test-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "780f284e0ebdf609724e48d6015990f26a433b2d11fcba8f8b08df791d1e6f9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
